{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00cdndice","title":"\u00cdndice"},{"location":"#indice","text":"","title":"\u00cdndice"},{"location":"Almacenamiento/mongodb/","text":"MongoDB MongoDB es una base de datos NoSQL en la que la informaci\u00f3n se organiza en documentos. Documentos Los documentos almacenan la informaci\u00f3n como pares clave-valor. Todas las claves son strings, y siempre hay que escribirlos con comillas; los valores pueden ser datos de cualquier tipo. JSON es un formato muy popular y f\u00e1cil de manejar, pero ocupa demasiado espacio. Por este motivo, MongoDB almacena los documentos de forma interna en un formato binario que llaman BSON, pero permite acceder a ellos con sintaxis JSON. Los documentos pueden contener otros documentos. Todos los documentos contienen un documento _id , que es un identificador \u00fanico. Las colecciones son conjuntos de documentos con contenidos similares. Por ejemplo, todos los documentos correspondientes a restaurantes se pueden almacenar en una colecci\u00f3n. Al contrario que en una tabla de SQL, podemos colocar documentos con claves totalmente diferentes en la misma colecci\u00f3n. Instalaci\u00f3n MongoDB se puede utilizar desde nuestro ordenador o en la nube: Instalaci\u00f3n local : descargar la versi\u00f3n gratuita en https://www.mongodb.com/try/download/community MongoDB Atlas : es un servicio en la nube de pago, con una opci\u00f3n gratuita que no requiere tarjeta de cr\u00e9dito Una vez configurada la base de datos, hay que instalar tambi\u00e9n uno de los servicios para acceder a ella: MongoDB Shell : l\u00ednea de comandos MongoDB Compass : interfaz gr\u00e1fica API : disponible para aplicaciones en Python, Java, JavaScript / NodeJS y otros En este tutorial emplearemos Compass para acceder a una base de datos de MongoDB en la nube. Los pasos a seguir est\u00e1n descritos en la gu\u00eda de instalaci\u00f3n . Visualizaci\u00f3n de datos Una vez configurada la base de datos y establecida la conexi\u00f3n desde Compass, podemos ver nuestras bases de datos y conexiones en el panel izquierdo, y pulsar en cualquiera de ellas para ver los documentos. Las colecciones se denotan como baseDeDatos.colecci\u00f3n : la colecci\u00f3n en la imagen es sample_restaurants.restaurants . Se pueden a\u00f1adir nuevos documentos con la opci\u00f3n Add Data . MongoDB les asigna autom\u00e1ticamente un ID \u00fanico. Los botones arriba a la derecha en la vista de documento permiten editar, copiar al portapapeles, clonar y borrar documentos. B\u00fasqueda de datos En este apartado vamos a utilizar el dataset sample_training.zips , en los datos de ejemplo de MongoDB. Contiene informaci\u00f3n sobre los c\u00f3digos postales de Estados Unidos: Al contrario que en SQL, las queries se escriben en formato JSON, indicando las claves por las que queremos filtrar y los valores que deben tomar. Por ejemplo, para buscar todos los c\u00f3digos postales en California escribimos {\"state\": \"CA\"} . Se pueden combinar varias condiciones en una b\u00fasqueda: En las queries se pueden usar operadores de comparaci\u00f3n como $eq (igual a) y $gt (mayor que): Para buscar los distritos postales con m\u00e1s de 1.000 pero menos de 10.000 habitantes, se podr\u00eda usar {\"pop\": { $gt: 1000, $lt: 10000}} . Si no incluimos ning\u00fan operador, MongoDB a\u00f1ade autom\u00e1ticamente el operador $eq : la query {\"state\": \"CA\"} es equivalente a {$eq: {\"state\": \"CA\"}} . Los operadores l\u00f3gicos son $and , $or , $not y $nor : $and : verdadero si se cumplen todas las condiciones $or : verdadero si al menos una de las condiciones se cumple $nor : verdadero si no se cumple ninguna de las condiciones dadas $not : verdadero si la condici\u00f3n es falsa Si una query incluye varias condiciones y ning\u00fan operador, MongoDB a\u00f1ade autom\u00e1ticamente el operador $and . Por ejemplo, las dos queries a continuaci\u00f3n son id\u00e9nticas, pero es mucho m\u00e1s c\u00f3modo usar la primera: {\"pop\": { $gt : 1000, $lt : 10000}} {\"$and\": [{\"pop\": {\"$gt\": 1000}}, {\"pop\": {\"$lt\": 10000}}]} L\u00ednea de comandos La interfaz gr\u00e1fica es \u00fatil para visualizar el conjunto de datos y realizar consultas y modificaciones r\u00e1pidas, pero s\u00f3lo da acceso a una peque\u00f1a parte de la funcionalidad de MongoDB; la mayor\u00eda de operaciones s\u00f3lo se pueden realizar desde la l\u00ednea de comandos. Compass incluye una l\u00ednea de comandos que se puede abrir pulsando sobre >_MONGOSH en la barra inferior. En este ejemplo ejecutamos la misma b\u00fasqueda que en el apartado anterior: # Mostrar todas las bases de datos show dbs # Seleccionar una use sample_training # Mostrar las colecciones show collections # Realizar una query db.zips.find ({ \"state\" : \"CA\" }) db.zips.find ({ \"state\" : \"CA\" }) .pretty () # Se muestran 20 resultados por pantalla. # Para ver los 20 siguientes escribir \"it\" (iterate) it Modificar datos Los documentos se pueden modificar desde la l\u00ednea de comandos con: updateMany : modifica todos los documentos que cumplan la condici\u00f3n updateOne : modifica s\u00f3lo el primer documento que encuentre. Como los documentos no est\u00e1n ordenados, el documento que se modificar\u00e1 es aleatorio En ambos casos, la sintaxis utiliza dos objetos JSON: el primero se utiliza para buscar los documentos que queremos cambiar, y el segundo define la modificaci\u00f3n. # A\u00f1adir 100 habitantes a todas las ciudades de California db.zips.updateMany ({ \"state\" : \"CA\" } , { \" $inc \" : { \"pop\" : 100 }}) Los operadores de modificaci\u00f3n son: $inc : aumenta un valor num\u00e9rico (o lo disminuye si la cantidad es negativa) $set : cambia el valor de una clave $unset : elimina una clave $push : a\u00f1ade un valor a un array Borrar datos Usar las funciones deleteOne o deleteMany . # Eliminar California del conjunto de datos db.zips.deleteMany ({ \"state\" : \"CA\" }) Tambi\u00e9n podemos borrar colecciones o bases de datos enteras con drop : # Eliminar la colecci\u00f3n zips db.zips.drop () # Eliminar la base de datos db.dropDatabase () Referencias Hemos elaborado este tutorial a partir del curso gratuito M001: MongoDB Basics en MongoDB University .","title":"MongoDB"},{"location":"Almacenamiento/mongodb/#mongodb","text":"MongoDB es una base de datos NoSQL en la que la informaci\u00f3n se organiza en documentos.","title":"MongoDB"},{"location":"Almacenamiento/mongodb/#documentos","text":"Los documentos almacenan la informaci\u00f3n como pares clave-valor. Todas las claves son strings, y siempre hay que escribirlos con comillas; los valores pueden ser datos de cualquier tipo. JSON es un formato muy popular y f\u00e1cil de manejar, pero ocupa demasiado espacio. Por este motivo, MongoDB almacena los documentos de forma interna en un formato binario que llaman BSON, pero permite acceder a ellos con sintaxis JSON. Los documentos pueden contener otros documentos. Todos los documentos contienen un documento _id , que es un identificador \u00fanico. Las colecciones son conjuntos de documentos con contenidos similares. Por ejemplo, todos los documentos correspondientes a restaurantes se pueden almacenar en una colecci\u00f3n. Al contrario que en una tabla de SQL, podemos colocar documentos con claves totalmente diferentes en la misma colecci\u00f3n.","title":"Documentos"},{"location":"Almacenamiento/mongodb/#instalacion","text":"MongoDB se puede utilizar desde nuestro ordenador o en la nube: Instalaci\u00f3n local : descargar la versi\u00f3n gratuita en https://www.mongodb.com/try/download/community MongoDB Atlas : es un servicio en la nube de pago, con una opci\u00f3n gratuita que no requiere tarjeta de cr\u00e9dito Una vez configurada la base de datos, hay que instalar tambi\u00e9n uno de los servicios para acceder a ella: MongoDB Shell : l\u00ednea de comandos MongoDB Compass : interfaz gr\u00e1fica API : disponible para aplicaciones en Python, Java, JavaScript / NodeJS y otros En este tutorial emplearemos Compass para acceder a una base de datos de MongoDB en la nube. Los pasos a seguir est\u00e1n descritos en la gu\u00eda de instalaci\u00f3n .","title":"Instalaci\u00f3n"},{"location":"Almacenamiento/mongodb/#visualizacion-de-datos","text":"Una vez configurada la base de datos y establecida la conexi\u00f3n desde Compass, podemos ver nuestras bases de datos y conexiones en el panel izquierdo, y pulsar en cualquiera de ellas para ver los documentos. Las colecciones se denotan como baseDeDatos.colecci\u00f3n : la colecci\u00f3n en la imagen es sample_restaurants.restaurants . Se pueden a\u00f1adir nuevos documentos con la opci\u00f3n Add Data . MongoDB les asigna autom\u00e1ticamente un ID \u00fanico. Los botones arriba a la derecha en la vista de documento permiten editar, copiar al portapapeles, clonar y borrar documentos.","title":"Visualizaci\u00f3n de datos"},{"location":"Almacenamiento/mongodb/#busqueda-de-datos","text":"En este apartado vamos a utilizar el dataset sample_training.zips , en los datos de ejemplo de MongoDB. Contiene informaci\u00f3n sobre los c\u00f3digos postales de Estados Unidos: Al contrario que en SQL, las queries se escriben en formato JSON, indicando las claves por las que queremos filtrar y los valores que deben tomar. Por ejemplo, para buscar todos los c\u00f3digos postales en California escribimos {\"state\": \"CA\"} . Se pueden combinar varias condiciones en una b\u00fasqueda: En las queries se pueden usar operadores de comparaci\u00f3n como $eq (igual a) y $gt (mayor que): Para buscar los distritos postales con m\u00e1s de 1.000 pero menos de 10.000 habitantes, se podr\u00eda usar {\"pop\": { $gt: 1000, $lt: 10000}} . Si no incluimos ning\u00fan operador, MongoDB a\u00f1ade autom\u00e1ticamente el operador $eq : la query {\"state\": \"CA\"} es equivalente a {$eq: {\"state\": \"CA\"}} . Los operadores l\u00f3gicos son $and , $or , $not y $nor : $and : verdadero si se cumplen todas las condiciones $or : verdadero si al menos una de las condiciones se cumple $nor : verdadero si no se cumple ninguna de las condiciones dadas $not : verdadero si la condici\u00f3n es falsa Si una query incluye varias condiciones y ning\u00fan operador, MongoDB a\u00f1ade autom\u00e1ticamente el operador $and . Por ejemplo, las dos queries a continuaci\u00f3n son id\u00e9nticas, pero es mucho m\u00e1s c\u00f3modo usar la primera: {\"pop\": { $gt : 1000, $lt : 10000}} {\"$and\": [{\"pop\": {\"$gt\": 1000}}, {\"pop\": {\"$lt\": 10000}}]}","title":"B\u00fasqueda de datos"},{"location":"Almacenamiento/mongodb/#linea-de-comandos","text":"La interfaz gr\u00e1fica es \u00fatil para visualizar el conjunto de datos y realizar consultas y modificaciones r\u00e1pidas, pero s\u00f3lo da acceso a una peque\u00f1a parte de la funcionalidad de MongoDB; la mayor\u00eda de operaciones s\u00f3lo se pueden realizar desde la l\u00ednea de comandos. Compass incluye una l\u00ednea de comandos que se puede abrir pulsando sobre >_MONGOSH en la barra inferior. En este ejemplo ejecutamos la misma b\u00fasqueda que en el apartado anterior: # Mostrar todas las bases de datos show dbs # Seleccionar una use sample_training # Mostrar las colecciones show collections # Realizar una query db.zips.find ({ \"state\" : \"CA\" }) db.zips.find ({ \"state\" : \"CA\" }) .pretty () # Se muestran 20 resultados por pantalla. # Para ver los 20 siguientes escribir \"it\" (iterate) it","title":"L\u00ednea de comandos"},{"location":"Almacenamiento/mongodb/#modificar-datos","text":"Los documentos se pueden modificar desde la l\u00ednea de comandos con: updateMany : modifica todos los documentos que cumplan la condici\u00f3n updateOne : modifica s\u00f3lo el primer documento que encuentre. Como los documentos no est\u00e1n ordenados, el documento que se modificar\u00e1 es aleatorio En ambos casos, la sintaxis utiliza dos objetos JSON: el primero se utiliza para buscar los documentos que queremos cambiar, y el segundo define la modificaci\u00f3n. # A\u00f1adir 100 habitantes a todas las ciudades de California db.zips.updateMany ({ \"state\" : \"CA\" } , { \" $inc \" : { \"pop\" : 100 }}) Los operadores de modificaci\u00f3n son: $inc : aumenta un valor num\u00e9rico (o lo disminuye si la cantidad es negativa) $set : cambia el valor de una clave $unset : elimina una clave $push : a\u00f1ade un valor a un array","title":"Modificar datos"},{"location":"Almacenamiento/mongodb/#borrar-datos","text":"Usar las funciones deleteOne o deleteMany . # Eliminar California del conjunto de datos db.zips.deleteMany ({ \"state\" : \"CA\" }) Tambi\u00e9n podemos borrar colecciones o bases de datos enteras con drop : # Eliminar la colecci\u00f3n zips db.zips.drop () # Eliminar la base de datos db.dropDatabase ()","title":"Borrar datos"},{"location":"Almacenamiento/mongodb/#referencias","text":"Hemos elaborado este tutorial a partir del curso gratuito M001: MongoDB Basics en MongoDB University .","title":"Referencias"},{"location":"Almacenamiento/mongodbatlas/","text":"MongoDB Atlas Atlas es un servicio en la nube que ofrece acceso a bases de datos MongoDB. Est\u00e1 organizado en tres niveles: Organizaciones -> Proyectos -> Bases de datos . Cada base de datos se almacena en un cl\u00faster. Los cl\u00fasters gratuitos ofrecen 512 MB de almacenamiento y r\u00e9plicas de los datos en tres m\u00e1quinas. Creaci\u00f3n de una base de datos Registrar una cuenta de usuario en https://www.mongodb.com/en/cloud/atlas/signup. Tambi\u00e9n se puede utilizar una cuenta de Google. Crear una nueva organizaci\u00f3n pulsando en Create an Organization . Elegir un nombre para la organizaci\u00f3n y seleccionar MongoDB Atlas como proveedor. En la pantalla siguiente se puede agregar a m\u00e1s usuarios a la organizaci\u00f3n. Dejarlo en blanco por ahora. Crear un proyecto vac\u00edo con Create Project . Pulsar Siguiente en el di\u00e1logo para a\u00f1adir nuevos miembros. Inicializar una base de datos dentro del proyecto con el bot\u00f3n Build a Database . Seleccionar el nivel gratuito. Elegir un proveedor de servicios (cualquiera sirve; en todos los casos manejaremos la base de datos desde la interfaz de Atlas y no se nos cobrar\u00e1 nada). Escoger la localizaci\u00f3n m\u00e1s cercana para los datos. Para seguir el tutorial oficial de MongoDB, elegir como nombre de cl\u00faster Sandbox . El cl\u00faster tardar\u00e1 unos minutos en crearse. Conectarse a la base de datos Para ver todas las bases de datos en el proyecto, pulsar en Databases en el men\u00fa a la izquierda de la p\u00e1gina. Pulsar en Connect para autorizar las conexiones a la base de datos desde el exterior. Como es un proyecto de prueba no nos preocupa que sea p\u00fablico: elegir Allow Access from Anywhere , y a continuaci\u00f3n Add IP Address . Por \u00faltimo, crear un usuario y contrase\u00f1a para acceder a la base de datos. En la p\u00e1gina siguiente se puede elegir entre varias opciones para conectarse a la base de datos: MongoDB Shell (l\u00ednea de comandos), MongoDB Compass (interfaz de usuario gr\u00e1fica), o desde una API para aplicaciones. Seleccionar la opci\u00f3n Compass y copiar el \"connection string\". Si no tenemos instalada la aplicaci\u00f3n, se puede descargar en la opci\u00f3n \"I do not have MongoDB Compass\". Por \u00faltimo, abrir la aplicaci\u00f3n Compass y pegar el string, reemplazando <username> y <password> por la contrase\u00f1a de usuario que elegimos en el paso anterior. Si todo ha salido bien, ya tenemos acceso al cl\u00faster desde Compass: Cargar el dataset de demostraci\u00f3n En el panel de control de MongoDB Atlas ( cloud.mongodb.com ), ir a la vista de bases de datos y elegir la opci\u00f3n Load Sample Dataset .","title":"MongoDB Atlas"},{"location":"Almacenamiento/mongodbatlas/#mongodb-atlas","text":"Atlas es un servicio en la nube que ofrece acceso a bases de datos MongoDB. Est\u00e1 organizado en tres niveles: Organizaciones -> Proyectos -> Bases de datos . Cada base de datos se almacena en un cl\u00faster. Los cl\u00fasters gratuitos ofrecen 512 MB de almacenamiento y r\u00e9plicas de los datos en tres m\u00e1quinas.","title":"MongoDB Atlas"},{"location":"Almacenamiento/mongodbatlas/#creacion-de-una-base-de-datos","text":"Registrar una cuenta de usuario en https://www.mongodb.com/en/cloud/atlas/signup. Tambi\u00e9n se puede utilizar una cuenta de Google. Crear una nueva organizaci\u00f3n pulsando en Create an Organization . Elegir un nombre para la organizaci\u00f3n y seleccionar MongoDB Atlas como proveedor. En la pantalla siguiente se puede agregar a m\u00e1s usuarios a la organizaci\u00f3n. Dejarlo en blanco por ahora. Crear un proyecto vac\u00edo con Create Project . Pulsar Siguiente en el di\u00e1logo para a\u00f1adir nuevos miembros. Inicializar una base de datos dentro del proyecto con el bot\u00f3n Build a Database . Seleccionar el nivel gratuito. Elegir un proveedor de servicios (cualquiera sirve; en todos los casos manejaremos la base de datos desde la interfaz de Atlas y no se nos cobrar\u00e1 nada). Escoger la localizaci\u00f3n m\u00e1s cercana para los datos. Para seguir el tutorial oficial de MongoDB, elegir como nombre de cl\u00faster Sandbox . El cl\u00faster tardar\u00e1 unos minutos en crearse.","title":"Creaci\u00f3n de una base de datos"},{"location":"Almacenamiento/mongodbatlas/#conectarse-a-la-base-de-datos","text":"Para ver todas las bases de datos en el proyecto, pulsar en Databases en el men\u00fa a la izquierda de la p\u00e1gina. Pulsar en Connect para autorizar las conexiones a la base de datos desde el exterior. Como es un proyecto de prueba no nos preocupa que sea p\u00fablico: elegir Allow Access from Anywhere , y a continuaci\u00f3n Add IP Address . Por \u00faltimo, crear un usuario y contrase\u00f1a para acceder a la base de datos. En la p\u00e1gina siguiente se puede elegir entre varias opciones para conectarse a la base de datos: MongoDB Shell (l\u00ednea de comandos), MongoDB Compass (interfaz de usuario gr\u00e1fica), o desde una API para aplicaciones. Seleccionar la opci\u00f3n Compass y copiar el \"connection string\". Si no tenemos instalada la aplicaci\u00f3n, se puede descargar en la opci\u00f3n \"I do not have MongoDB Compass\". Por \u00faltimo, abrir la aplicaci\u00f3n Compass y pegar el string, reemplazando <username> y <password> por la contrase\u00f1a de usuario que elegimos en el paso anterior. Si todo ha salido bien, ya tenemos acceso al cl\u00faster desde Compass:","title":"Conectarse a la base de datos"},{"location":"Almacenamiento/mongodbatlas/#cargar-el-dataset-de-demostracion","text":"En el panel de control de MongoDB Atlas ( cloud.mongodb.com ), ir a la vista de bases de datos y elegir la opci\u00f3n Load Sample Dataset .","title":"Cargar el dataset de demostraci\u00f3n"},{"location":"Almacenamiento/spark-dataframes/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Spark Dataframes Instalaci\u00f3n de Spark Estas instrucciones son para cualquier sistema Ubuntu/Debian, y funcionan tambi\u00e9n en Google Colab. Si Spark ya est\u00e1 instalado no hay que ejecutar este bloque. ! apt - get install openjdk - 8 - jdk - headless - qq > / dev / null ! wget - q https : // downloads . apache . org / spark / spark - 3.1.2 / spark - 3.1.2 - bin - hadoop3 .2 . tgz ! tar xf spark - 3.1.2 - bin - hadoop3 .2 . tgz ! pip install findspark import os import findspark os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-3.1.2-bin-hadoop3.2\" findspark . init () from pyspark.sql import SparkSession spark = SparkSession . builder \\ . master ( \"local\" ) \\ . appName ( \"Colab\" ) \\ . config ( 'spark.ui.port' , '4050' ) \\ . getOrCreate () Creaci\u00f3n de dataframes # Desde CSV ratings = spark . read . csv ( filename , header = True , inferSchema = True ) # Desde JSON ratings = spark . read . json ( filename , header = True , inferSchema = True ) # A partir de una DF de Pandas ratings = spark . createDataFrame ( df ) # Manualmente id_usuarios = spark . createDataFrame ([ 12 , 45 , 3 , 26 , 8 ], \"int\" ) . toDF ( \"user_id\" ) Funciones .select(...) : extrae una o m\u00e1s columnas de un DataFrame. df.select('Model', 'ScreenSize').show() .filter(...) : selecciona las filas que cumplen una condici\u00f3n. df.filter(df.Year > 2015).show() .groupBy(...) : aplica una funci\u00f3n de agregaci\u00f3n a los valores de una o varias columnas. df.groupBy('RAM').count().show() .orderBy(...) : ordena valores. df.orderBy(\u2018Weight').show() df1.union(df2) : concatena dos dataframes que tienen el mismo schema df1.join(df2, type=...) : junta dos DataFrames, utilizando uniones SQL (inner, etc.) .fillna(...) : da valores a valores vac\u00edos. .dropna() : elimina filas que contienen valores nulos. .dropDuplicates() : elimina filas duplicadas. .summary() , .describe() : muestran valores estad\u00edsticos descriptivos. .freqItems(...) : cuenta el n\u00famero de veces que aparecen los valores en una columna. .show() : Muestra las primeras l\u00edneas. .collect() : Recolecta todos los valores de los nodos de trabajo y los vuelca en un objeto Python. Cuidado con DataFrames muy grandes. .take(...) : Similar a collect pero solo recupera un subconjunto. .toPandas() : Convierte el DataFrame de Spark en un DataFrame de Pandas. SQL Tambi\u00e9n se puede acceder a una dataframe utilizando SQL. Hay que crear una vista con createOrReplaceTempView, y despu\u00e9s pasar el comando SQL como un string. datosDF . createOrReplaceTempView ( \u2018 miTabla \u2019 ) spark . sql ( '''SELECT Model, Year, RAM, HDD, ScreenSize FROM miTabla''' ) . show ()","title":"Spark dataframes"},{"location":"Almacenamiento/spark-dataframes/#spark-dataframes","text":"","title":"Spark Dataframes"},{"location":"Almacenamiento/spark-dataframes/#instalacion-de-spark","text":"Estas instrucciones son para cualquier sistema Ubuntu/Debian, y funcionan tambi\u00e9n en Google Colab. Si Spark ya est\u00e1 instalado no hay que ejecutar este bloque. ! apt - get install openjdk - 8 - jdk - headless - qq > / dev / null ! wget - q https : // downloads . apache . org / spark / spark - 3.1.2 / spark - 3.1.2 - bin - hadoop3 .2 . tgz ! tar xf spark - 3.1.2 - bin - hadoop3 .2 . tgz ! pip install findspark import os import findspark os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-3.1.2-bin-hadoop3.2\" findspark . init () from pyspark.sql import SparkSession spark = SparkSession . builder \\ . master ( \"local\" ) \\ . appName ( \"Colab\" ) \\ . config ( 'spark.ui.port' , '4050' ) \\ . getOrCreate ()","title":"Instalaci\u00f3n de Spark"},{"location":"Almacenamiento/spark-dataframes/#creacion-de-dataframes","text":"# Desde CSV ratings = spark . read . csv ( filename , header = True , inferSchema = True ) # Desde JSON ratings = spark . read . json ( filename , header = True , inferSchema = True ) # A partir de una DF de Pandas ratings = spark . createDataFrame ( df ) # Manualmente id_usuarios = spark . createDataFrame ([ 12 , 45 , 3 , 26 , 8 ], \"int\" ) . toDF ( \"user_id\" )","title":"Creaci\u00f3n de dataframes"},{"location":"Almacenamiento/spark-dataframes/#funciones","text":".select(...) : extrae una o m\u00e1s columnas de un DataFrame. df.select('Model', 'ScreenSize').show() .filter(...) : selecciona las filas que cumplen una condici\u00f3n. df.filter(df.Year > 2015).show() .groupBy(...) : aplica una funci\u00f3n de agregaci\u00f3n a los valores de una o varias columnas. df.groupBy('RAM').count().show() .orderBy(...) : ordena valores. df.orderBy(\u2018Weight').show() df1.union(df2) : concatena dos dataframes que tienen el mismo schema df1.join(df2, type=...) : junta dos DataFrames, utilizando uniones SQL (inner, etc.) .fillna(...) : da valores a valores vac\u00edos. .dropna() : elimina filas que contienen valores nulos. .dropDuplicates() : elimina filas duplicadas. .summary() , .describe() : muestran valores estad\u00edsticos descriptivos. .freqItems(...) : cuenta el n\u00famero de veces que aparecen los valores en una columna. .show() : Muestra las primeras l\u00edneas. .collect() : Recolecta todos los valores de los nodos de trabajo y los vuelca en un objeto Python. Cuidado con DataFrames muy grandes. .take(...) : Similar a collect pero solo recupera un subconjunto. .toPandas() : Convierte el DataFrame de Spark en un DataFrame de Pandas.","title":"Funciones"},{"location":"Almacenamiento/spark-dataframes/#sql","text":"Tambi\u00e9n se puede acceder a una dataframe utilizando SQL. Hay que crear una vista con createOrReplaceTempView, y despu\u00e9s pasar el comando SQL como un string. datosDF . createOrReplaceTempView ( \u2018 miTabla \u2019 ) spark . sql ( '''SELECT Model, Year, RAM, HDD, ScreenSize FROM miTabla''' ) . show ()","title":"SQL"},{"location":"Almacenamiento/spark/","text":"Spark Spark es un sistema de computaci\u00f3n distribuida. Tiene la capacidad de ejecutarse en memoria RAM, lo que lo hace adecuado para procesamiento de datos en tiempo real. Est\u00e1 escrito en Scala, y tambi\u00e9n se puede usar con Java, Python, R y SQL. Se puede utilizar en combinaci\u00f3n con casi cualquier sistema de almacenamiento (Hadoop, Cassandra, etc.). Las aplicaciones se pueden ejecutar localmente, en modo interactivo, o en un cl\u00faster, enviando tareas a los ordenadores del cl\u00faster mediante un scheduler. Los schedulers m\u00e1s habituales son YARN (NodeManager + ResourceManager) y Spark Standalone (Spark Worker + Spark Master); es aconsejable usar YARN. Al contrario que MapReduce, que siempre emplea dos pasos de computaci\u00f3n, Spark puede dividir el c\u00e1lculo en cualquier n\u00famero de pasos, organizados mediante una DAG (gr\u00e1fica ac\u00edclica dirigida), y no necesita escribir en disco los resultados de las tareas intermedias. Spark trabaja con tres estructuras de datos: RDD (resilient distributed dataset): tablas de s\u00f3lo lectura que garantizan la resistencia a fallos Dataframes : tablas estructuradas en las que cada columna tiene un nombre y un tipo de datos. No confundir con los dataframes de Pandas Datasets : son un t\u00e9rmino medio entre RDD y dataframe Lo m\u00e1s habitual en PySpark es usar dataframes, porque las RDD tienen rendimiento bajo y los datasets no est\u00e1n disponibles en Python. La librer\u00eda SparkML permite usar Spark para aprendizaje autom\u00e1tico. Instalaci\u00f3n Estos comandos instalan PySpark y Hadoop en un notebook de Jupyter ejecut\u00e1ndose sobre Debian o Ubuntu (incluyendo Google Colab). ! apt - get install openjdk - 8 - jdk - headless - qq > / dev / null ! wget - q https : // downloads . apache . org / spark / spark - 3.1.2 / spark - 3.1.2 - bin - hadoop3 .2 . tgz ! tar xf spark - 3.1.2 - bin - hadoop3 .2 . tgz ! pip install findspark import os import findspark os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-3.1.2-bin-hadoop3.2\" findspark . init () Spark Sessions Una SparkSession es el punto de entrada de una aplicaci\u00f3n Spark. Se puede crear con SparkSession.builder(). Las sesiones interactivas crean autom\u00e1ticamente una SparkSession, llamada spark. from pyspark.sql import SparkSession spark = SparkSession . builder \\ . master ( \"local\" ) \\ . appName ( \"Colab\" ) \\ . config ( 'spark.ui.port' , '4050' ) \\ . getOrCreate () Note En versiones anteriores de Spark, hab\u00eda varios puntos de entrada para cada parte de la aplicaci\u00f3n: SparkContext para RDD, SQLContext para SQL, etc. SparkSession los combina todos en uno. Estructura de la aplicaci\u00f3n Una aplicaci\u00f3n Spark se asigna a un \u00fanico proceso de controlador y un conjunto de procesos de ejecutor, distribuidos a trav\u00e9s del cl\u00faster, que se crean y destruyen autom\u00e1ticamente en funci\u00f3n de las necesidades del programa. El proceso de controlador gestiona el flujo de trabajo y lo distribuye en tareas, y los ejecutores ejecutan las tareas. Un ejecutor puede ejecutar varias tareas a la vez. RDD Los RDD (resilient distributed dataset) son tablas de s\u00f3lo lectura que contienen informaci\u00f3n no estructurada. Los datos pueden estar distribuidos en varios nodos, y la informaci\u00f3n se puede recuperar autom\u00e1ticamente si uno de los nodos falla. Los c\u00e1lculos sobre RDD se realizan mediante: transformaciones : funciones que devuelven otro RDD. Ej.: map, filter, flatmap, sample, union, intersection acciones : funciones que devuelven un valor. Ej.: reduce, collect, count Spark es \"perezoso\" (lazy): las transformaciones s\u00f3lo se ejecutan cuando sus resultados son necesarios para calcular alguna acci\u00f3n. Dataframes Los dataframes son conjuntos de datos de s\u00f3lo lectura organizados en tablas. Est\u00e1n creados a partir de RDDs, pero ofrecen mejor rendimiento y m\u00e1s capacidades. Al contrario que los RDDs, los dataframes contienen datos estructurados: cada columna tiene un nombre y s\u00f3lo puede contener un tipo de datos. Para crear un dataframe a partir de un archivo de datos, usar SparkSession.read : Los dataframes se pueden manejar con SQL o con funciones. Ver notebook con ejemplos SparkML La librer\u00eda por defecto para aprendizaje autom\u00e1tico en Spark es SparkML ( pyspark.ml ). Se puede utilizar con RDDs o dataframes. from pyspark.ml.recommendation import ALS from pyspark.ml.evaluation import RegressionEvaluator ( training , test ) = ratings . randomSplit ( \\ [ 0.8 , 0.2 \\ ]) # Entrenamos el modelo als = ALS ( maxIter = 5 , regParam = 0.01 , userCol = \"user_id\" , itemCol = \"id\" , ratingCol = \"rating\" , coldStartStrategy = \"drop\" ) model = als . fit ( training ) # Evaluamos el modelo con RMSE predictions = model . transform ( test ) evaluator = RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"rating\" , predictionCol = \"prediction\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root-mean-square error = \" + str ( rmse )) # Generar las 10 mejores recomendaciones para un subconjunto de usuarios userSubsetRecs = model . recommendForUserSubset ( usuario_EP , 5 )","title":"Spark"},{"location":"Almacenamiento/spark/#spark","text":"Spark es un sistema de computaci\u00f3n distribuida. Tiene la capacidad de ejecutarse en memoria RAM, lo que lo hace adecuado para procesamiento de datos en tiempo real. Est\u00e1 escrito en Scala, y tambi\u00e9n se puede usar con Java, Python, R y SQL. Se puede utilizar en combinaci\u00f3n con casi cualquier sistema de almacenamiento (Hadoop, Cassandra, etc.). Las aplicaciones se pueden ejecutar localmente, en modo interactivo, o en un cl\u00faster, enviando tareas a los ordenadores del cl\u00faster mediante un scheduler. Los schedulers m\u00e1s habituales son YARN (NodeManager + ResourceManager) y Spark Standalone (Spark Worker + Spark Master); es aconsejable usar YARN. Al contrario que MapReduce, que siempre emplea dos pasos de computaci\u00f3n, Spark puede dividir el c\u00e1lculo en cualquier n\u00famero de pasos, organizados mediante una DAG (gr\u00e1fica ac\u00edclica dirigida), y no necesita escribir en disco los resultados de las tareas intermedias. Spark trabaja con tres estructuras de datos: RDD (resilient distributed dataset): tablas de s\u00f3lo lectura que garantizan la resistencia a fallos Dataframes : tablas estructuradas en las que cada columna tiene un nombre y un tipo de datos. No confundir con los dataframes de Pandas Datasets : son un t\u00e9rmino medio entre RDD y dataframe Lo m\u00e1s habitual en PySpark es usar dataframes, porque las RDD tienen rendimiento bajo y los datasets no est\u00e1n disponibles en Python. La librer\u00eda SparkML permite usar Spark para aprendizaje autom\u00e1tico.","title":"Spark"},{"location":"Almacenamiento/spark/#instalacion","text":"Estos comandos instalan PySpark y Hadoop en un notebook de Jupyter ejecut\u00e1ndose sobre Debian o Ubuntu (incluyendo Google Colab). ! apt - get install openjdk - 8 - jdk - headless - qq > / dev / null ! wget - q https : // downloads . apache . org / spark / spark - 3.1.2 / spark - 3.1.2 - bin - hadoop3 .2 . tgz ! tar xf spark - 3.1.2 - bin - hadoop3 .2 . tgz ! pip install findspark import os import findspark os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-3.1.2-bin-hadoop3.2\" findspark . init ()","title":"Instalaci\u00f3n"},{"location":"Almacenamiento/spark/#spark-sessions","text":"Una SparkSession es el punto de entrada de una aplicaci\u00f3n Spark. Se puede crear con SparkSession.builder(). Las sesiones interactivas crean autom\u00e1ticamente una SparkSession, llamada spark. from pyspark.sql import SparkSession spark = SparkSession . builder \\ . master ( \"local\" ) \\ . appName ( \"Colab\" ) \\ . config ( 'spark.ui.port' , '4050' ) \\ . getOrCreate () Note En versiones anteriores de Spark, hab\u00eda varios puntos de entrada para cada parte de la aplicaci\u00f3n: SparkContext para RDD, SQLContext para SQL, etc. SparkSession los combina todos en uno.","title":"Spark Sessions"},{"location":"Almacenamiento/spark/#estructura-de-la-aplicacion","text":"Una aplicaci\u00f3n Spark se asigna a un \u00fanico proceso de controlador y un conjunto de procesos de ejecutor, distribuidos a trav\u00e9s del cl\u00faster, que se crean y destruyen autom\u00e1ticamente en funci\u00f3n de las necesidades del programa. El proceso de controlador gestiona el flujo de trabajo y lo distribuye en tareas, y los ejecutores ejecutan las tareas. Un ejecutor puede ejecutar varias tareas a la vez.","title":"Estructura de la aplicaci\u00f3n"},{"location":"Almacenamiento/spark/#rdd","text":"Los RDD (resilient distributed dataset) son tablas de s\u00f3lo lectura que contienen informaci\u00f3n no estructurada. Los datos pueden estar distribuidos en varios nodos, y la informaci\u00f3n se puede recuperar autom\u00e1ticamente si uno de los nodos falla. Los c\u00e1lculos sobre RDD se realizan mediante: transformaciones : funciones que devuelven otro RDD. Ej.: map, filter, flatmap, sample, union, intersection acciones : funciones que devuelven un valor. Ej.: reduce, collect, count Spark es \"perezoso\" (lazy): las transformaciones s\u00f3lo se ejecutan cuando sus resultados son necesarios para calcular alguna acci\u00f3n.","title":"RDD"},{"location":"Almacenamiento/spark/#dataframes","text":"Los dataframes son conjuntos de datos de s\u00f3lo lectura organizados en tablas. Est\u00e1n creados a partir de RDDs, pero ofrecen mejor rendimiento y m\u00e1s capacidades. Al contrario que los RDDs, los dataframes contienen datos estructurados: cada columna tiene un nombre y s\u00f3lo puede contener un tipo de datos. Para crear un dataframe a partir de un archivo de datos, usar SparkSession.read : Los dataframes se pueden manejar con SQL o con funciones. Ver notebook con ejemplos","title":"Dataframes"},{"location":"Almacenamiento/spark/#sparkml","text":"La librer\u00eda por defecto para aprendizaje autom\u00e1tico en Spark es SparkML ( pyspark.ml ). Se puede utilizar con RDDs o dataframes. from pyspark.ml.recommendation import ALS from pyspark.ml.evaluation import RegressionEvaluator ( training , test ) = ratings . randomSplit ( \\ [ 0.8 , 0.2 \\ ]) # Entrenamos el modelo als = ALS ( maxIter = 5 , regParam = 0.01 , userCol = \"user_id\" , itemCol = \"id\" , ratingCol = \"rating\" , coldStartStrategy = \"drop\" ) model = als . fit ( training ) # Evaluamos el modelo con RMSE predictions = model . transform ( test ) evaluator = RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"rating\" , predictionCol = \"prediction\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root-mean-square error = \" + str ( rmse )) # Generar las 10 mejores recomendaciones para un subconjunto de usuarios userSubsetRecs = model . recommendForUserSubset ( usuario_EP , 5 )","title":"SparkML"},{"location":"Almacenamiento/sql/","text":"SQL SQL vs noSQL Las bases de datos se dividen en relacionales (SQL): almacenan la informaci\u00f3n en tablas relacionadas entre s\u00ed no relacionales (noSQL) Todas las bases de datos relacionales se pueden manejar con el lenguaje SQL (Structured Query Language). Hay diferencias en la sintaxis de SQL dependiendo del modelo de base de datos que estemos usando. Google Cloud Estos apuntes son para la base de datos de Oracle, que utilizamos en 1\u00ba. En BigQuery hay varias diferencias: BigQuery usa una versi\u00f3n de SQL que llaman \"Standard SQL\". Algunos de los comandos son diferentes, como las conversiones de tipos. Se puede usar SQL normal seleccionando More -> Query Settings -> Legacy SQL en el editor de queries Los tipos de variable son diferentes: STRING , INT , NUMERIC , BOOL , DATE / DATETIME , y otros D\u00f3nde practicar LiveSQL - entorno interactivo de Oracle, con tutoriales para ver el estado actual de la base de datos, pulsar el bot\u00f3n Find SQLFiddle - otro entorno interactivo, va m\u00e1s fluido que el de Oracle. Utilizar el panel de la izquierda para crear las tablas e insertar datos, el de la derecha es s\u00f3lo para queries Imagen de VirtualBox - como LA M\u00c1QUINA de Susy pero cien veces m\u00e1s r\u00e1pida Instalaci\u00f3n local: base de datos y SQL developer en SQL Developer se puede ver el estado actual de todas las tablas en el panel Conexiones, a la izquierda de la pantalla. Si no aparece, ir al men\u00fa Ventana -> Restaurar configuraci\u00f3n de f\u00e1brica. SQL Developer y LiveSQL ejecutan s\u00f3lo el c\u00f3digo seleccionado. Si no hay nada seleccionado, ejecutan el script entero. Ejemplos Tabla de ejemplo : http://sqlfiddle.com/#!4/5497c. Los ejemplos en este documento se pueden ejecutar sobre esta tabla. Ejercicio de m\u00e9dicos y pacientes de Susy : http://sqlfiddle.com/#!4/4e935d3 SQLZoo : https://sqlzoo.net/. Ejercicios para practicar queries Sintaxis Los identificadores (nombres de tablas, columnas, etc.) deben empezar con una letra y pueden contener letras, n\u00fameros y barras bajas, pero no espacios ni puntos. Los identificadores no distinguen entre may\u00fasculas y min\u00fasculas, pero internamente est\u00e1n siempre en may\u00fasculas; esto es importante a la hora de usar SELECT . Los valores contenidos en las tablas s\u00ed distinguen entre may\u00fasculas y min\u00fasculas. La convenci\u00f3n que usa casi todo el mundo es escribir los comandos en may\u00fasculas y todo lo dem\u00e1s en min\u00fasculas. La que usa Susy es poner los nombres de tabla en may\u00fasculas y lo dem\u00e1s en min\u00fasculas. Poner un punto y coma despu\u00e9s de cada frase. Al insertar VARCHAR2 y DATE usar comillas simples ( ' ), no dobles. Comentarios: -- (una l\u00ednea) y /* */ (varias l\u00edneas). Si un comentario es lo \u00faltimo que hay en un archivo, Oracle va a darnos un error. Crear tablas Usar CREATE TABLE . CREATE TABLE empresas ( nombre VARCHAR2 ( 20 ) PRIMARY KEY ); CREATE TABLE empleados ( dni NUMBER ( 8 ) PRIMARY KEY , nombre VARCHAR2 ( 20 ) NOT NULL , empresa VARCHAR2 ( 20 ) REFERENCES empresas ( nombre ), sueldo NUMBER ( 6 ) NOT NULL ); Las columnas de las tablas se escriben separadas por comas. Escribir primero el nombre, seguido del tipo y las constraints como NOT NULL . Para ver las columnas que contiene una tabla usar DESCRIBE tabla , y para ver todos sus elementos, SELECT * FROM tabla . La tabla user_tables es una tabla interna de Oracle que contiene los nombres y datos de todas las tablas que hemos definido. Para ver una lista de todas nuestras tablas, usar SELECT table_name FROM user_tables . Una tabla se puede eliminar con DROP TABLE tabla . Si la tabla contiene constraints, hay que usar DROP TABLE tabla CASCADE CONSTRAINTS . Las tablas s\u00f3lo se pueden eliminar una a una; para borrar todas nuestras tablas a la vez seguir estos pasos: Ejecutar el comando SELECT 'DROP TABLE \"' || TABLE_NAME || '\" CASCADE CONSTRAINTS;' FROM user_tables; Esta query nos da como resultado una serie de comandos, cada uno de los cuales elimina una tabla. Copiar y pegar todos estos comandos y ejecutarlos para eliminar todas las tablas Modificar tablas Para a\u00f1adir, modificar o eliminar columnas o constraints, usar ALTER TABLE con ADD , MODIFY , DROP o RENAME . Algunos de estos comandos no funcionar\u00e1n si la tabla que queremos modificar ya contiene datos. Por ejemplo, s\u00f3lo podr\u00edamos a\u00f1adir columnas que fueran null . -- A\u00f1adir columna ALTER TABLE empleados ADD empleo VARCHAR2 ( 100 ); -- Modificar tipo de datos ALTER TABLE empleados MODIFY dni VARCHAR2 ( 9 ); -- Eliminar columna ALTER TABLE empleados DROP COLUMN nombre ; -- Eliminar constraint (s\u00f3lo si la declaramos fuera de l\u00ednea con un nombre) ALTER TABLE ejemplo DROP CONSTRAINT const_name ; -- Renombrar ALTER TABLE empleados RENAME COLUMN sueldo TO salario ; Obtener informaci\u00f3n sobre las tablas Lista de todas las tablas: SELECT table_name FROM user_tables; Todas las propiedades de todas las tablas: SELECT * FROM user_tables; Tipos de joins Hay cuatro tipos de joins: INNER JOIN : muestra s\u00f3lo los elementos en los que la condici\u00f3n de uni\u00f3n se cumple. FULL JOIN : muestra todas las columnas de todas las tablas, aunque la condici\u00f3n de uni\u00f3n no se cumpla. LEFT JOIN : muestra todas las columnas de la tabla izquierda, y s\u00f3lo las de la tabla derecha que cumplen la condici\u00f3n de uni\u00f3n. RIGHT JOIN : muestra todas las columnas de la tabla derecha, y s\u00f3lo las de la tabla izquierda que cumplen la condici\u00f3n de uni\u00f3n. El join por defecto es INNER JOIN . En los otros tres tipos (outer joins), cuando la condici\u00f3n no se cumple para una fila, esa fila se completa con null . Constraints Las constraints imponen condiciones a los valores que puede contener una columna. Se pueden declarar en l\u00ednea o fuera de l\u00ednea: -- Constraint en l\u00ednea dni NUMBER ( 8 ) PRIMARY KEY -- Constraint fuera de l\u00ednea (abreviada) dni NUMBER ( 8 ), PRIMARY KEY ( dni ) -- Constraint fuera de l\u00ednea (con nombre) dni NUMBER ( 8 ), CONSTRAINT pk_dni PRIMARY KEY ( dni ) Las constraints fuera de l\u00ednea pueden tener nombre. Estos nombres nos permiten seleccionar la constraint si queremos modificarla o eliminarla posteriormente. No puede haber dos constraints con el mismo nombre. Las constraints s\u00f3lo se pueden usar dentro de un CREATE TABLE , ALTER TABLE , CREATE VIEW O ALTER VIEW . Hay seis tipos, s\u00f3lo desarrollo los que hemos visto en clase: primary key Marca la columna como clave primaria. La clave primaria debe ser \u00fanica, pero se puede crear una PK m\u00faltiple con PRIMARY KEY (columna1, columna2) . Esto s\u00f3lo funciona con notaci\u00f3n fuera de l\u00ednea. foreign key Indica que la columna proviene de otra tabla. La tabla de origen se indica con REFERENCES : -- Podemos omitir el tipo de dato, porque Oracle lo copiar\u00e1 de la tabla origen cod_empresa REFERENCES empresas ( cod_empresa ) -- Si la columna tiene el mismo nombre en ambas tablas, no es necesario indicarlo. cod_empresa REFERENCES empresas -- Si queremos hacerlo fuera de l\u00ednea: COD_EMPRESA number , CONSTRAINT fk_tabla1_cod_empresa FOREIGN KEY ( cod_empresa ) REFERENCES empresas -- Tener en cuenta que -- 1. hay que declarar la clave antes de aplicar la constraint -- 2. el c\u00f3digo de constraint debe ser siempre \u00fanico, por eso a\u00f1adimos el nombre de la tabla actual -- 3. al igual que en el caso anterior, el nombre de columna se puede omitir, pero s\u00f3lo si es el mismo nombre en ambas tablas Las FK pueden ser null. M\u00e1s informaci\u00f3n sobre interacciones entre FK y otras constraints: https://docs.oracle.com/cd/B28359_01/appdev.111/b28424/adfns_constraints.htm#sthref549 not null Indica que el valor de la columna es obligatorio y no se puede dejar en blanco. Esta constraint siempre se tiene que declarar con notaci\u00f3n en l\u00ednea. nombre VARCHAR2 ( 20 ) NOT NULL Todas las PK son tambi\u00e9n not null, y no hay que indicarlo manualmente. unique Indica que la columna no puede contener valores repetidos. Todas las primary key son unique, y no es necesario indicarlo. correo_electronico VARCHAR2 ( 50 ) UNIQUE Tipos de datos Los tipos de datos son diferentes en cada modelo de base de datos. Los que vamos a utilizar son: VARCHAR2(n) : texto con un tama\u00f1o m\u00e1ximo de n bytes (n letras, excepto si utilizamos caracteres especiales). Utilizarlo siempre en vez de CHAR o VARCHAR . El valor m\u00e1ximo de n es 4000, para datos m\u00e1s grandes usar tipos LOB . NUMBER(n, m) : almacena n\u00fameros. n es el total de cifras (incluyendo decimales), y m el n\u00famero de decimales. DATE : almacena fecha y hora. Para introducirlas usar el formato TO_DATE('24-01-1548', 'DD-MM-YYYY . Manual de la funci\u00f3n TO_DATE . Lista completa de tipos A\u00f1adir datos Usar INSERT INTO tabla (columna1, columna2, ...) VALUES (valor1, valor2, ...); . INSERT INTO empleados ( dni , nombre , empresa , sueldo ) VALUES ( 53845739 , 'Pablo' , 'Google' , 5000 ); La lista de columnas se puede omitir si damos los valores en el mismo orden en que declaramos las columnas al crear la tabla. INSERT INTO empleados VALUES ( 3823734 , 'Marcos' , 'Amazon' , 5500 ); Se puede insertar varios valores a la vez con INSERT ALL . No ahorra nada de tiempo al escribirlo, pero es m\u00e1s eficiente que enviar los comandos uno a uno. Al final de un INSERT ALL es obligatorio incluir un SELECT ; la opci\u00f3n m\u00e1s com\u00fan es usar SELECT * FROM dual , que no hace nada. Los valores que se insertan van todos seguidos, sin coma o punto y coma. INSERT ALL INTO empleados ( dni , nombre , empresa , sueldo ) VALUES ( 53845739 , 'Pablo' , 'Google' , 5000 ) INTO empleados VALUES ( 3823734 , 'Marcos' , 'Amazon' , 5500 ) SELECT * FROM DUAL ; Modificar datos UPDATE permite cambiar datos ya introducidos en la tabla, y DELETE se usa para eliminarlos. Por defecto se aplican a todas las filas de la tabla; tambi\u00e9n se pueden combinar con WHERE si s\u00f3lo queremos que afecten a algunas filas. UPDATE empleados SET sueldo = sueldo + 1000 ; DELETE FROM empleados WHERE empresa = 'Google' ; Selecciones El comando SELECT ... FROM ... nos permite seleccionar los elementos que cumplan una condici\u00f3n dada. SELECT * FROM empleados ; Se pueden pasar varias cl\u00e1usulas adicionales, en este orden : SELECT DISTINCT ... FROM ... JOIN ... WHERE ... GROUP BY ... ORDER BY ... . La cl\u00e1usula FROM es obligatoria, todas las dem\u00e1s son opcionales. SELECT DISTINCT nombre , empresa FROM empleados WHERE sueldo > 2000 GROUP BY empresa , nombre ORDER BY nombre ; DISTINCT : no muestra los resultados repetidos. JOIN : se utiliza para enlazar datos que est\u00e1n en tablas distintas pero comparten alg\u00fan campo. Ver abajo WHERE : muestra s\u00f3lo los resultados que cumplen una condici\u00f3n. Ver abajo GROUP BY : muestra los resultados agrupados seg\u00fan los valores \u00fanicos de una de las columnas. ORDER BY : muestra los resultados ordenados seg\u00fan los valores de una o varias columnas. Se le pueden pasar dos par\u00e1metros adicionales: * ASC \u00f3 DESC (orden ascendiente o descendiente, por defecto ASC ) * NULLS FIRST \u00f3 NULLS LAST (por defecto NULLS LAST ) Lista de todas las tablas: SELECT table_name FROM user_tables; El comando SELECT se puede anidar. Combinaci\u00f3n de selecciones En SQL podemos combinar los resultados de varias selecciones. Para que se puedan aplicar estas operaciones, los resultados deben tener el mismo n\u00famero de columnas, y estas columnas deben tener tipos compatibles. selecci\u00f3n1 UNION selecci\u00f3n2 devuelve la uni\u00f3n de las selecciones: todos los elementos que est\u00e1n al menos en una de ellas. Si hay filas que est\u00e1n en ambas selecciones s\u00f3lo devuelve una; si queremos que muestre las dos hay que usar UNION ALL . selecci\u00f3n1 INTERSECT selecci\u00f3n2 devuelve la intersecci\u00f3n de las selecciones: los elementos que est\u00e1n en ambas. selecci\u00f3n1 MINUS selecci\u00f3n2 devuelve los elementos que est\u00e1n en la primera selecci\u00f3n pero no en la segunda. Condicionales Los condicionales en SQL se representan con la cl\u00e1usula WHERE . Se puede utilizar dentro de un comando SELECT , INSERT , UPDATE o DELETE . La sintaxis del comando es WHERE condici\u00f3n , donde la condici\u00f3n puede tomar una de estas formas: sueldo = 1000 , sueldo <= 1000 (\u00f3 >= ) y sueldo BETWEEN 2000 AND 2500 . SELECT nombre , sueldo FROM empleados WHERE sueldo >= 5000 ; Podemos combinar varias condiciones usando AND , OR y NOT . Si tenemos m\u00e1s de dos condiciones es aconsejable usar par\u00e9ntesis para dejar claro en qu\u00e9 orden queremos que se combinen. SELECT nombre , sueldo FROM empleados WHERE ( sueldo > 3000 ) AND ( NOT empresa = 'Google' ); Group by El comando GROUP BY columna agrupa en una sola fila todos los elementos que comparten el mismo valor de la columna. Como este comando va a agrupar varias filas en una, tenemos que utilizar alguna funci\u00f3n de agregaci\u00f3n, como SUM , en los datos que queramos mostrar. Ver http://sqlfiddle.com/#!4/9b7433/4 -- Este comando va a darnos error, porque GROUP BY muestra una fila por empresa, -- y al haber varios empleados por cada empresa no sabr\u00eda qu\u00e9 nombre mostrar SELECT nombre FROM empleados GROUP BY empresa ; -- Hay que usar una funci\u00f3n de agregaci\u00f3n: SELECT sum ( sueldo ) FROM empleados GROUP BY empresa ; -- Podemos mostrar la columna del GROUP BY en el resultado del SELECT: SELECT sum ( sueldo ), empresa FROM empleados GROUP BY empresa ; Joins La cl\u00e1usula JOIN se utiliza para enlazar datos que est\u00e1n distribuidos entre dos o m\u00e1s tablas pero comparten alg\u00fan campo. Por ejemplo, si tenemos una tabla para empleados y otra con datos de empresas, podr\u00edamos hacer un join para obtener en una sola query toda la informaci\u00f3n de cada empleado y de la empresa en la que trabaja. Ver la tabla de ejemplo para joins: http://sqlfiddle.com/#!4/4f1f5a/1. La sintaxis del comando es SELECT columnas FROM tabla1 JOIN tabla2 ON columnasIguales . columnas son las columnas que queremos que aparezcan en el resultado, y columnasIguales son las columnas cuyos contenidos deben coincidir. -- Este comando muestra en una sola fila la informaci\u00f3n de cada empleado -- y toda la informaci\u00f3n del departamento al que pertenece SELECT * FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept ; -- Id\u00e9ntico al anterior, pero s\u00f3lo muestra empleados en el departamento 1 SELECT * FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept WHERE departamentos . cod_dept = 1 ; -- Siempre que hacemos ``SELECT *`` en un join vamos a tener columnas repetidas. -- Podemos elegir manualmente qu\u00e9 columnas queremos mostrar SELECT empleados . nombre , empleados . apellidos , departamentos . nombre FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept ; -- Podemos hacer varios joins en una sola consulta SELECT * FROM empleados JOIN e_bonos ON empleados . dni = e_bonos . dni JOIN bonos ON e_bonos . cod_bono = bonos . cod_bono ; Cualquier join se puede declarar de forma equivalente con un simple WHERE . En general es preferible utilizar joins, porque son m\u00e1s legibles y m\u00e1s f\u00e1ciles de optimizar. SELECT suppliers . supplier_id , suppliers . supplier_name , orders . order_date FROM suppliers , orders WHERE suppliers . supplier_id = orders . supplier_id ; -- es equivalente a: SELECT suppliers . supplier_id , suppliers . supplier_name , orders . order_date FROM suppliers JOIN orders ON suppliers . supplier_id = orders . supplier_id ; Tipos de joins Hay cuatro tipos de joins: INNER JOIN : muestra s\u00f3lo los elementos en los que la condici\u00f3n de uni\u00f3n se cumple. FULL JOIN : muestra todas las columnas de todas las tablas, aunque la condici\u00f3n de uni\u00f3n no se cumpla. LEFT JOIN : muestra todas las columnas de la tabla izquierda, y s\u00f3lo las de la tabla derecha que cumplen la condici\u00f3n de uni\u00f3n. RIGHT JOIN : muestra todas las columnas de la tabla derecha, y s\u00f3lo las de la tabla izquierda que cumplen la condici\u00f3n de uni\u00f3n. El join por defecto es INNER JOIN . En los otros tres tipos (outer joins), cuando la condici\u00f3n no se cumple para una fila, esa fila se completa con null . Funciones Por ahora s\u00f3lo hemos visto funciones de agregaci\u00f3n , que toman como argumento un conjunto de filas y devuelven un solo valor. Estas funciones se utilizan dentro de un SELECT , y se pueden combinar con todas las cl\u00e1usulas de un SELECT , como WHERE , GROUP BY , etc. SUM() : calcula la suma de los valores, que deben ser n\u00fameros MAX() , MIN() : devuelve el m\u00e1ximo o m\u00ednimo de los valores AVG() : calcula la media de los valores COUNT() : devuelve el n\u00famero de filas en una tabla. COUNT(*) devuelve el n\u00famero total de filas, COUNT(ALL columna) devuelve todas las filas excepto si su valor en columna es null , y COUNT(DISTINCT columna) devuelve el n\u00famero de valores \u00fanicos en columna , ignorando tambi\u00e9n los null -- Obtiene la suma de los sueldos de los empleados de Amazon SELECT SUM(sueldo) FROM empleados WHERE empresa = 'Amazon'; -- Cuenta el n\u00famero total de empleados SELECT COUNT(*) FROM empleados; -- Cuenta el n\u00famero de empresas que pagan m\u00e1s de 5000 euros a alg\u00fan empleado SELECT COUNT(DISTINCT empresa) FROM empleados WHERE sueldo >= 5000; Tener en cuenta que no podemos hacer una query que contenga, por ejemplo, nombre y sum(sueldo) , porque el n\u00famero de resultados ser\u00eda diferente ( sum devuelve un solo resultado, y nombre devuelve uno por fila). Una forma de solucionar esto es utilizar GROUP BY , ya que GROUP BY fuerza a la query a devolver un n\u00famero fijo de resultados (uno por grupo). SELECT departamentos . descripcion , SUM ( empleados . sueldo ) FROM departamentos , empleados WHERE departamentos . cod_dept = empleados . cod_dept GROUP BY departamentos . descripcion ; Hay tambi\u00e9n otros tipos de funciones . Adem\u00e1s, las bases de datos de Oracle incluyen un lenguaje de programaci\u00f3n propio para complementar a SQL, llamado PL.","title":"SQL"},{"location":"Almacenamiento/sql/#sql","text":"","title":"SQL"},{"location":"Almacenamiento/sql/#sql-vs-nosql","text":"Las bases de datos se dividen en relacionales (SQL): almacenan la informaci\u00f3n en tablas relacionadas entre s\u00ed no relacionales (noSQL) Todas las bases de datos relacionales se pueden manejar con el lenguaje SQL (Structured Query Language). Hay diferencias en la sintaxis de SQL dependiendo del modelo de base de datos que estemos usando.","title":"SQL vs noSQL"},{"location":"Almacenamiento/sql/#google-cloud","text":"Estos apuntes son para la base de datos de Oracle, que utilizamos en 1\u00ba. En BigQuery hay varias diferencias: BigQuery usa una versi\u00f3n de SQL que llaman \"Standard SQL\". Algunos de los comandos son diferentes, como las conversiones de tipos. Se puede usar SQL normal seleccionando More -> Query Settings -> Legacy SQL en el editor de queries Los tipos de variable son diferentes: STRING , INT , NUMERIC , BOOL , DATE / DATETIME , y otros","title":"Google Cloud"},{"location":"Almacenamiento/sql/#donde-practicar","text":"LiveSQL - entorno interactivo de Oracle, con tutoriales para ver el estado actual de la base de datos, pulsar el bot\u00f3n Find SQLFiddle - otro entorno interactivo, va m\u00e1s fluido que el de Oracle. Utilizar el panel de la izquierda para crear las tablas e insertar datos, el de la derecha es s\u00f3lo para queries Imagen de VirtualBox - como LA M\u00c1QUINA de Susy pero cien veces m\u00e1s r\u00e1pida Instalaci\u00f3n local: base de datos y SQL developer en SQL Developer se puede ver el estado actual de todas las tablas en el panel Conexiones, a la izquierda de la pantalla. Si no aparece, ir al men\u00fa Ventana -> Restaurar configuraci\u00f3n de f\u00e1brica. SQL Developer y LiveSQL ejecutan s\u00f3lo el c\u00f3digo seleccionado. Si no hay nada seleccionado, ejecutan el script entero.","title":"D\u00f3nde practicar"},{"location":"Almacenamiento/sql/#ejemplos","text":"Tabla de ejemplo : http://sqlfiddle.com/#!4/5497c. Los ejemplos en este documento se pueden ejecutar sobre esta tabla. Ejercicio de m\u00e9dicos y pacientes de Susy : http://sqlfiddle.com/#!4/4e935d3 SQLZoo : https://sqlzoo.net/. Ejercicios para practicar queries","title":"Ejemplos"},{"location":"Almacenamiento/sql/#sintaxis","text":"Los identificadores (nombres de tablas, columnas, etc.) deben empezar con una letra y pueden contener letras, n\u00fameros y barras bajas, pero no espacios ni puntos. Los identificadores no distinguen entre may\u00fasculas y min\u00fasculas, pero internamente est\u00e1n siempre en may\u00fasculas; esto es importante a la hora de usar SELECT . Los valores contenidos en las tablas s\u00ed distinguen entre may\u00fasculas y min\u00fasculas. La convenci\u00f3n que usa casi todo el mundo es escribir los comandos en may\u00fasculas y todo lo dem\u00e1s en min\u00fasculas. La que usa Susy es poner los nombres de tabla en may\u00fasculas y lo dem\u00e1s en min\u00fasculas. Poner un punto y coma despu\u00e9s de cada frase. Al insertar VARCHAR2 y DATE usar comillas simples ( ' ), no dobles. Comentarios: -- (una l\u00ednea) y /* */ (varias l\u00edneas). Si un comentario es lo \u00faltimo que hay en un archivo, Oracle va a darnos un error.","title":"Sintaxis"},{"location":"Almacenamiento/sql/#crear-tablas","text":"Usar CREATE TABLE . CREATE TABLE empresas ( nombre VARCHAR2 ( 20 ) PRIMARY KEY ); CREATE TABLE empleados ( dni NUMBER ( 8 ) PRIMARY KEY , nombre VARCHAR2 ( 20 ) NOT NULL , empresa VARCHAR2 ( 20 ) REFERENCES empresas ( nombre ), sueldo NUMBER ( 6 ) NOT NULL ); Las columnas de las tablas se escriben separadas por comas. Escribir primero el nombre, seguido del tipo y las constraints como NOT NULL . Para ver las columnas que contiene una tabla usar DESCRIBE tabla , y para ver todos sus elementos, SELECT * FROM tabla . La tabla user_tables es una tabla interna de Oracle que contiene los nombres y datos de todas las tablas que hemos definido. Para ver una lista de todas nuestras tablas, usar SELECT table_name FROM user_tables . Una tabla se puede eliminar con DROP TABLE tabla . Si la tabla contiene constraints, hay que usar DROP TABLE tabla CASCADE CONSTRAINTS . Las tablas s\u00f3lo se pueden eliminar una a una; para borrar todas nuestras tablas a la vez seguir estos pasos: Ejecutar el comando SELECT 'DROP TABLE \"' || TABLE_NAME || '\" CASCADE CONSTRAINTS;' FROM user_tables; Esta query nos da como resultado una serie de comandos, cada uno de los cuales elimina una tabla. Copiar y pegar todos estos comandos y ejecutarlos para eliminar todas las tablas","title":"Crear tablas"},{"location":"Almacenamiento/sql/#modificar-tablas","text":"Para a\u00f1adir, modificar o eliminar columnas o constraints, usar ALTER TABLE con ADD , MODIFY , DROP o RENAME . Algunos de estos comandos no funcionar\u00e1n si la tabla que queremos modificar ya contiene datos. Por ejemplo, s\u00f3lo podr\u00edamos a\u00f1adir columnas que fueran null . -- A\u00f1adir columna ALTER TABLE empleados ADD empleo VARCHAR2 ( 100 ); -- Modificar tipo de datos ALTER TABLE empleados MODIFY dni VARCHAR2 ( 9 ); -- Eliminar columna ALTER TABLE empleados DROP COLUMN nombre ; -- Eliminar constraint (s\u00f3lo si la declaramos fuera de l\u00ednea con un nombre) ALTER TABLE ejemplo DROP CONSTRAINT const_name ; -- Renombrar ALTER TABLE empleados RENAME COLUMN sueldo TO salario ;","title":"Modificar tablas"},{"location":"Almacenamiento/sql/#obtener-informacion-sobre-las-tablas","text":"Lista de todas las tablas: SELECT table_name FROM user_tables; Todas las propiedades de todas las tablas: SELECT * FROM user_tables;","title":"Obtener informaci\u00f3n sobre las tablas"},{"location":"Almacenamiento/sql/#tipos-de-joins","text":"Hay cuatro tipos de joins: INNER JOIN : muestra s\u00f3lo los elementos en los que la condici\u00f3n de uni\u00f3n se cumple. FULL JOIN : muestra todas las columnas de todas las tablas, aunque la condici\u00f3n de uni\u00f3n no se cumpla. LEFT JOIN : muestra todas las columnas de la tabla izquierda, y s\u00f3lo las de la tabla derecha que cumplen la condici\u00f3n de uni\u00f3n. RIGHT JOIN : muestra todas las columnas de la tabla derecha, y s\u00f3lo las de la tabla izquierda que cumplen la condici\u00f3n de uni\u00f3n. El join por defecto es INNER JOIN . En los otros tres tipos (outer joins), cuando la condici\u00f3n no se cumple para una fila, esa fila se completa con null .","title":"Tipos de joins"},{"location":"Almacenamiento/sql/#constraints","text":"Las constraints imponen condiciones a los valores que puede contener una columna. Se pueden declarar en l\u00ednea o fuera de l\u00ednea: -- Constraint en l\u00ednea dni NUMBER ( 8 ) PRIMARY KEY -- Constraint fuera de l\u00ednea (abreviada) dni NUMBER ( 8 ), PRIMARY KEY ( dni ) -- Constraint fuera de l\u00ednea (con nombre) dni NUMBER ( 8 ), CONSTRAINT pk_dni PRIMARY KEY ( dni ) Las constraints fuera de l\u00ednea pueden tener nombre. Estos nombres nos permiten seleccionar la constraint si queremos modificarla o eliminarla posteriormente. No puede haber dos constraints con el mismo nombre. Las constraints s\u00f3lo se pueden usar dentro de un CREATE TABLE , ALTER TABLE , CREATE VIEW O ALTER VIEW . Hay seis tipos, s\u00f3lo desarrollo los que hemos visto en clase:","title":"Constraints"},{"location":"Almacenamiento/sql/#primary-key","text":"Marca la columna como clave primaria. La clave primaria debe ser \u00fanica, pero se puede crear una PK m\u00faltiple con PRIMARY KEY (columna1, columna2) . Esto s\u00f3lo funciona con notaci\u00f3n fuera de l\u00ednea.","title":"primary key"},{"location":"Almacenamiento/sql/#foreign-key","text":"Indica que la columna proviene de otra tabla. La tabla de origen se indica con REFERENCES : -- Podemos omitir el tipo de dato, porque Oracle lo copiar\u00e1 de la tabla origen cod_empresa REFERENCES empresas ( cod_empresa ) -- Si la columna tiene el mismo nombre en ambas tablas, no es necesario indicarlo. cod_empresa REFERENCES empresas -- Si queremos hacerlo fuera de l\u00ednea: COD_EMPRESA number , CONSTRAINT fk_tabla1_cod_empresa FOREIGN KEY ( cod_empresa ) REFERENCES empresas -- Tener en cuenta que -- 1. hay que declarar la clave antes de aplicar la constraint -- 2. el c\u00f3digo de constraint debe ser siempre \u00fanico, por eso a\u00f1adimos el nombre de la tabla actual -- 3. al igual que en el caso anterior, el nombre de columna se puede omitir, pero s\u00f3lo si es el mismo nombre en ambas tablas Las FK pueden ser null. M\u00e1s informaci\u00f3n sobre interacciones entre FK y otras constraints: https://docs.oracle.com/cd/B28359_01/appdev.111/b28424/adfns_constraints.htm#sthref549","title":"foreign key"},{"location":"Almacenamiento/sql/#not-null","text":"Indica que el valor de la columna es obligatorio y no se puede dejar en blanco. Esta constraint siempre se tiene que declarar con notaci\u00f3n en l\u00ednea. nombre VARCHAR2 ( 20 ) NOT NULL Todas las PK son tambi\u00e9n not null, y no hay que indicarlo manualmente.","title":"not null"},{"location":"Almacenamiento/sql/#unique","text":"Indica que la columna no puede contener valores repetidos. Todas las primary key son unique, y no es necesario indicarlo. correo_electronico VARCHAR2 ( 50 ) UNIQUE","title":"unique"},{"location":"Almacenamiento/sql/#tipos-de-datos","text":"Los tipos de datos son diferentes en cada modelo de base de datos. Los que vamos a utilizar son: VARCHAR2(n) : texto con un tama\u00f1o m\u00e1ximo de n bytes (n letras, excepto si utilizamos caracteres especiales). Utilizarlo siempre en vez de CHAR o VARCHAR . El valor m\u00e1ximo de n es 4000, para datos m\u00e1s grandes usar tipos LOB . NUMBER(n, m) : almacena n\u00fameros. n es el total de cifras (incluyendo decimales), y m el n\u00famero de decimales. DATE : almacena fecha y hora. Para introducirlas usar el formato TO_DATE('24-01-1548', 'DD-MM-YYYY . Manual de la funci\u00f3n TO_DATE . Lista completa de tipos","title":"Tipos de datos"},{"location":"Almacenamiento/sql/#anadir-datos","text":"Usar INSERT INTO tabla (columna1, columna2, ...) VALUES (valor1, valor2, ...); . INSERT INTO empleados ( dni , nombre , empresa , sueldo ) VALUES ( 53845739 , 'Pablo' , 'Google' , 5000 ); La lista de columnas se puede omitir si damos los valores en el mismo orden en que declaramos las columnas al crear la tabla. INSERT INTO empleados VALUES ( 3823734 , 'Marcos' , 'Amazon' , 5500 ); Se puede insertar varios valores a la vez con INSERT ALL . No ahorra nada de tiempo al escribirlo, pero es m\u00e1s eficiente que enviar los comandos uno a uno. Al final de un INSERT ALL es obligatorio incluir un SELECT ; la opci\u00f3n m\u00e1s com\u00fan es usar SELECT * FROM dual , que no hace nada. Los valores que se insertan van todos seguidos, sin coma o punto y coma. INSERT ALL INTO empleados ( dni , nombre , empresa , sueldo ) VALUES ( 53845739 , 'Pablo' , 'Google' , 5000 ) INTO empleados VALUES ( 3823734 , 'Marcos' , 'Amazon' , 5500 ) SELECT * FROM DUAL ;","title":"A\u00f1adir datos"},{"location":"Almacenamiento/sql/#modificar-datos","text":"UPDATE permite cambiar datos ya introducidos en la tabla, y DELETE se usa para eliminarlos. Por defecto se aplican a todas las filas de la tabla; tambi\u00e9n se pueden combinar con WHERE si s\u00f3lo queremos que afecten a algunas filas. UPDATE empleados SET sueldo = sueldo + 1000 ; DELETE FROM empleados WHERE empresa = 'Google' ;","title":"Modificar datos"},{"location":"Almacenamiento/sql/#selecciones","text":"El comando SELECT ... FROM ... nos permite seleccionar los elementos que cumplan una condici\u00f3n dada. SELECT * FROM empleados ; Se pueden pasar varias cl\u00e1usulas adicionales, en este orden : SELECT DISTINCT ... FROM ... JOIN ... WHERE ... GROUP BY ... ORDER BY ... . La cl\u00e1usula FROM es obligatoria, todas las dem\u00e1s son opcionales. SELECT DISTINCT nombre , empresa FROM empleados WHERE sueldo > 2000 GROUP BY empresa , nombre ORDER BY nombre ; DISTINCT : no muestra los resultados repetidos. JOIN : se utiliza para enlazar datos que est\u00e1n en tablas distintas pero comparten alg\u00fan campo. Ver abajo WHERE : muestra s\u00f3lo los resultados que cumplen una condici\u00f3n. Ver abajo GROUP BY : muestra los resultados agrupados seg\u00fan los valores \u00fanicos de una de las columnas. ORDER BY : muestra los resultados ordenados seg\u00fan los valores de una o varias columnas. Se le pueden pasar dos par\u00e1metros adicionales: * ASC \u00f3 DESC (orden ascendiente o descendiente, por defecto ASC ) * NULLS FIRST \u00f3 NULLS LAST (por defecto NULLS LAST ) Lista de todas las tablas: SELECT table_name FROM user_tables; El comando SELECT se puede anidar.","title":"Selecciones"},{"location":"Almacenamiento/sql/#combinacion-de-selecciones","text":"En SQL podemos combinar los resultados de varias selecciones. Para que se puedan aplicar estas operaciones, los resultados deben tener el mismo n\u00famero de columnas, y estas columnas deben tener tipos compatibles. selecci\u00f3n1 UNION selecci\u00f3n2 devuelve la uni\u00f3n de las selecciones: todos los elementos que est\u00e1n al menos en una de ellas. Si hay filas que est\u00e1n en ambas selecciones s\u00f3lo devuelve una; si queremos que muestre las dos hay que usar UNION ALL . selecci\u00f3n1 INTERSECT selecci\u00f3n2 devuelve la intersecci\u00f3n de las selecciones: los elementos que est\u00e1n en ambas. selecci\u00f3n1 MINUS selecci\u00f3n2 devuelve los elementos que est\u00e1n en la primera selecci\u00f3n pero no en la segunda.","title":"Combinaci\u00f3n de selecciones"},{"location":"Almacenamiento/sql/#condicionales","text":"Los condicionales en SQL se representan con la cl\u00e1usula WHERE . Se puede utilizar dentro de un comando SELECT , INSERT , UPDATE o DELETE . La sintaxis del comando es WHERE condici\u00f3n , donde la condici\u00f3n puede tomar una de estas formas: sueldo = 1000 , sueldo <= 1000 (\u00f3 >= ) y sueldo BETWEEN 2000 AND 2500 . SELECT nombre , sueldo FROM empleados WHERE sueldo >= 5000 ; Podemos combinar varias condiciones usando AND , OR y NOT . Si tenemos m\u00e1s de dos condiciones es aconsejable usar par\u00e9ntesis para dejar claro en qu\u00e9 orden queremos que se combinen. SELECT nombre , sueldo FROM empleados WHERE ( sueldo > 3000 ) AND ( NOT empresa = 'Google' );","title":"Condicionales"},{"location":"Almacenamiento/sql/#group-by","text":"El comando GROUP BY columna agrupa en una sola fila todos los elementos que comparten el mismo valor de la columna. Como este comando va a agrupar varias filas en una, tenemos que utilizar alguna funci\u00f3n de agregaci\u00f3n, como SUM , en los datos que queramos mostrar. Ver http://sqlfiddle.com/#!4/9b7433/4 -- Este comando va a darnos error, porque GROUP BY muestra una fila por empresa, -- y al haber varios empleados por cada empresa no sabr\u00eda qu\u00e9 nombre mostrar SELECT nombre FROM empleados GROUP BY empresa ; -- Hay que usar una funci\u00f3n de agregaci\u00f3n: SELECT sum ( sueldo ) FROM empleados GROUP BY empresa ; -- Podemos mostrar la columna del GROUP BY en el resultado del SELECT: SELECT sum ( sueldo ), empresa FROM empleados GROUP BY empresa ;","title":"Group by"},{"location":"Almacenamiento/sql/#joins","text":"La cl\u00e1usula JOIN se utiliza para enlazar datos que est\u00e1n distribuidos entre dos o m\u00e1s tablas pero comparten alg\u00fan campo. Por ejemplo, si tenemos una tabla para empleados y otra con datos de empresas, podr\u00edamos hacer un join para obtener en una sola query toda la informaci\u00f3n de cada empleado y de la empresa en la que trabaja. Ver la tabla de ejemplo para joins: http://sqlfiddle.com/#!4/4f1f5a/1. La sintaxis del comando es SELECT columnas FROM tabla1 JOIN tabla2 ON columnasIguales . columnas son las columnas que queremos que aparezcan en el resultado, y columnasIguales son las columnas cuyos contenidos deben coincidir. -- Este comando muestra en una sola fila la informaci\u00f3n de cada empleado -- y toda la informaci\u00f3n del departamento al que pertenece SELECT * FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept ; -- Id\u00e9ntico al anterior, pero s\u00f3lo muestra empleados en el departamento 1 SELECT * FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept WHERE departamentos . cod_dept = 1 ; -- Siempre que hacemos ``SELECT *`` en un join vamos a tener columnas repetidas. -- Podemos elegir manualmente qu\u00e9 columnas queremos mostrar SELECT empleados . nombre , empleados . apellidos , departamentos . nombre FROM empleados JOIN departamentos ON empleados . cod_dept = departamentos . cod_dept ; -- Podemos hacer varios joins en una sola consulta SELECT * FROM empleados JOIN e_bonos ON empleados . dni = e_bonos . dni JOIN bonos ON e_bonos . cod_bono = bonos . cod_bono ; Cualquier join se puede declarar de forma equivalente con un simple WHERE . En general es preferible utilizar joins, porque son m\u00e1s legibles y m\u00e1s f\u00e1ciles de optimizar. SELECT suppliers . supplier_id , suppliers . supplier_name , orders . order_date FROM suppliers , orders WHERE suppliers . supplier_id = orders . supplier_id ; -- es equivalente a: SELECT suppliers . supplier_id , suppliers . supplier_name , orders . order_date FROM suppliers JOIN orders ON suppliers . supplier_id = orders . supplier_id ;","title":"Joins"},{"location":"Almacenamiento/sql/#tipos-de-joins_1","text":"Hay cuatro tipos de joins: INNER JOIN : muestra s\u00f3lo los elementos en los que la condici\u00f3n de uni\u00f3n se cumple. FULL JOIN : muestra todas las columnas de todas las tablas, aunque la condici\u00f3n de uni\u00f3n no se cumpla. LEFT JOIN : muestra todas las columnas de la tabla izquierda, y s\u00f3lo las de la tabla derecha que cumplen la condici\u00f3n de uni\u00f3n. RIGHT JOIN : muestra todas las columnas de la tabla derecha, y s\u00f3lo las de la tabla izquierda que cumplen la condici\u00f3n de uni\u00f3n. El join por defecto es INNER JOIN . En los otros tres tipos (outer joins), cuando la condici\u00f3n no se cumple para una fila, esa fila se completa con null .","title":"Tipos de joins"},{"location":"Almacenamiento/sql/#funciones","text":"Por ahora s\u00f3lo hemos visto funciones de agregaci\u00f3n , que toman como argumento un conjunto de filas y devuelven un solo valor. Estas funciones se utilizan dentro de un SELECT , y se pueden combinar con todas las cl\u00e1usulas de un SELECT , como WHERE , GROUP BY , etc. SUM() : calcula la suma de los valores, que deben ser n\u00fameros MAX() , MIN() : devuelve el m\u00e1ximo o m\u00ednimo de los valores AVG() : calcula la media de los valores COUNT() : devuelve el n\u00famero de filas en una tabla. COUNT(*) devuelve el n\u00famero total de filas, COUNT(ALL columna) devuelve todas las filas excepto si su valor en columna es null , y COUNT(DISTINCT columna) devuelve el n\u00famero de valores \u00fanicos en columna , ignorando tambi\u00e9n los null -- Obtiene la suma de los sueldos de los empleados de Amazon SELECT SUM(sueldo) FROM empleados WHERE empresa = 'Amazon'; -- Cuenta el n\u00famero total de empleados SELECT COUNT(*) FROM empleados; -- Cuenta el n\u00famero de empresas que pagan m\u00e1s de 5000 euros a alg\u00fan empleado SELECT COUNT(DISTINCT empresa) FROM empleados WHERE sueldo >= 5000; Tener en cuenta que no podemos hacer una query que contenga, por ejemplo, nombre y sum(sueldo) , porque el n\u00famero de resultados ser\u00eda diferente ( sum devuelve un solo resultado, y nombre devuelve uno por fila). Una forma de solucionar esto es utilizar GROUP BY , ya que GROUP BY fuerza a la query a devolver un n\u00famero fijo de resultados (uno por grupo). SELECT departamentos . descripcion , SUM ( empleados . sueldo ) FROM departamentos , empleados WHERE departamentos . cod_dept = empleados . cod_dept GROUP BY departamentos . descripcion ; Hay tambi\u00e9n otros tipos de funciones . Adem\u00e1s, las bases de datos de Oracle incluyen un lenguaje de programaci\u00f3n propio para complementar a SQL, llamado PL.","title":"Funciones"},{"location":"Cosas%20varias/Linux/","text":"L\u00ednea de comandos de Linux La mayor\u00eda de sistemas operativos basados en Linux contienen un conjunto de programas peque\u00f1os conocidos como GNU coreutils, que dan acceso a la funcionalidad b\u00e1sica del sistema: manejo de archivos, edici\u00f3n de texto, etc. Las opciones pueden ser cortas ( ls -l ) o largas ( ls --version ). Las opciones cortas se pueden dar todas a la vez: ls -lsr . Al contrario que Windows, Unix distingue may\u00fasculas de min\u00fasculas. Pulsar Ctrl-C interrumpe la ejecuci\u00f3n de cualquier programa. Sistema de archivos Al igual que Windows, Unix emplea un sistema de archivos jer\u00e1rquico. Windows Linux Directorio ra\u00edz C:\\ / Otros discos duros / USB / etc. D:\\, E:\\, etc. /mnt/sda1, /mnt/sda2, etc. Directorio casa del usuario C:\\Usuarios\\nombre_usuario /home/ nombre_usuario Atajo: ~ Escritorio C:\\Usuarios\\nombre\\Escritorio ~/Desktop Archivos de programa C:\\Archivos de programa C:\\Archivos de programa (x64) /usr/bin El s\u00edmbolo ~ se puede escribir con AltGr + 4 y hace referencia al directorio \u201ccasa\u201d del usuario actual. Las rutas pueden ser absolutas (/usr/ejemplo.txt) relativas al directorio actual (ejemplo.txt) relativas al directorio casa del usuario (~/ejemplo.txt) Navegaci\u00f3n pwd (present working directory): mostrar la ruta completa del directorio actual ls directorio (list): muestra todos los archivos en el directorio dado. Si no le pasamos ning\u00fan argumento, utiliza el directorio actual ls -l incluye informaci\u00f3n sobre el tama\u00f1o y permisos de los archivos ls -a incluye archivos ocultos ls -R muestra tambi\u00e9n los archivos en subdirectorios cd directorio (change directory): se mueve al directorio indicado Para ir al directorio superior, cd .. Archivos y directorios touch nombre : crea un archivo vac\u00edo mkdir directorio (make directory): crea un directorio vac\u00edo cp origen destino : copia el archivo origen al destino mv origen destino : mueve el archivo origen al destino rm archivo (remove): elimina un archivo rmdir elimina directorios Para borrar todos los archivos y subdirectorios, rm \u2013rf *.* (mirar bien en qu\u00e9 directorio estamos antes de ejecutar esto) Ayuda man comando (manual): muestra instrucciones de uso de un comando Los manuales suelen tener muchas p\u00e1ginas. Para salir, pulsar q Manejo de texto La mayor\u00eda de programas en Linux utilizan texto plano como entrada y salida. cat archivo : abre un archivo de texto y muestra su contenido. Si pasamos varios archivos, los concatena y despu\u00e9s los muestra todos juntos grep texto archivo : busca el texto dado en un archivo sed archivo operaci\u00f3n (stream editor): realiza b\u00fasquedas y sustituciones sencillas. El primer argumento es el archivo, y el segundo es un string cuya primera letra indica la operaci\u00f3n que queremos hacer: s (substitute): busca una expresi\u00f3n y la sustituye por otra. La sintaxis es \u2018 s/ regexp / sustituci\u00f3n / g \u2019 donde regexp es la expresi\u00f3n regular que buscamos, sustituci\u00f3n es el texto con el que la reemplazamos y la g (opcional) indica que queremos reemplazar todas las veces que aparezca la expresi\u00f3n y no s\u00f3lo la primera. d (delete): borra el texto. Se suele usar en combinaci\u00f3n con un n\u00famero de l\u00ednea: 1d elimina la primera l\u00ednea. Normalmente la salida de un programa se muestra por pantalla. El operador | (pipe) permite concatenar comandos, pasando la salida de un comando como entrada al siguiente. Tambi\u00e9n podemos almacenar la salida del comando en un archivo: >> a\u00f1ade la informaci\u00f3n al final del archivo, y > lo sobreescribe. Por ejemplo, si tenemos una serie de precios que usan la coma como separador decimal y queremos cambiarlas a puntos: Editores de texto Casi todas las distribuciones de Linux incluyen al menos uno de estos editores de texto gr\u00e1ficos, que se pueden usar directamente desde la consola: nano : editor sencillo. Para abrir un archivo escribir en la consola nano nombre_archivo , y al terminar de editarlo salir con Ctrl-X. vi : editor potente pero sorprendentemente dif\u00edcil de usar. Ver https://www.openvim.com/ Secure Shell Normalmente vamos a querer utilizar estos comandos para controlar m\u00e1quinas de forma remota. La forma est\u00e1ndar de abrir un terminal en un ordenador diferente es ssh (secure shell). Antes de poder usarlo hay que instalar y configurar un servidor ssh en el ordenador remoto: apt install openssh-server . Una vez hecho esto, nos podemos conectar a \u00e9l desde cualquier otro ordenador con ssh *direcci\u00f3nIP* , y ejecutar comandos con ssh *servidor comando* . Los servicios en la nube (Google Cloud, etc.) ya manejan todo esto por nosotros, y permiten abrir una terminal en la m\u00e1quina remota pulsando un bot\u00f3n: Instalar programas Las m\u00e1quinas virtuales suelen usar sistemas operativos con muy pocos programas instalados, para ahorrar espacio. Para instalar lo que necesitemos hay que usar un \u201cgestor de paquetes\u201d: En Debian, Ubuntu y similares, apt : apt get apt remove. Esto mantiene la configuraci\u00f3n del programa; para borrarla usar apt purge apt update En Alpine y otras distribuciones minimalistas, apk : apk add apk del apk update Estos comandos requieren permisos de superusuario. (Ver secci\u00f3n siguiente) Comandos de superusuario En Unix hay usuarios normales, con permisos restringidos, y superusuarios (tambi\u00e9n llamados root), que tienen control absoluto sobre el sistema. En general se usa siempre una cuenta de usuario normal, para evitar romper cosas por accidente. Si queremos usar un comando que requiere permisos de superusuario, podemos usar la utilidad sudo , que ejecutar\u00e1 solamente ese comando como root (despu\u00e9s de pedirnos la contrase\u00f1a). Cron jobs Cron se usa para que el sistema ejecute autom\u00e1ticamente scripts cada cierto tiempo. Cada usuario tiene una lista de trabajos cron, que se puede editar con crontab -e . En cada l\u00ednea de la lista hay cinco n\u00fameros, que representan el minuto, hora, d\u00eda, mes y d\u00eda de la semana en que se debe ejecutar el script, y se pueden reemplazar por un * para que se ejecute siempre. Por ejemplo, para ejecutar un comando todos los d\u00edas a las 9:24 de la noche, escribimos 24 21 * * * . A continuaci\u00f3n se escribe el comando a ejecutar. Los scripts se ejecutar\u00e1n con los permisos del usuario al que corresponde la cron file. Tambi\u00e9n hay una cron file del sistema, que puede ejecutar scripts con los permisos de cualquier usuario. Para que el script funcione hay que marcarlo como ejecutable: sudo chmod +x backup.sh . Alternativas Hay tambi\u00e9n sistemas basados en Linux que no utilizan las coreutils de GNU. La mayor\u00eda de los comandos en este documento seguir\u00e1n funcionando, pero algunos tendr\u00e1n menos opciones o directamente no existir\u00e1n. A veces podremos instalar los programas que faltan con un gestor de aplicaciones. Busybox: es un sustituto de las coreutils para sistemas que tienen que ocupar muy poco espacio, como Alpine Linux. Contiene la mayor\u00eda de los programas, pero suelen tener menos opciones. Mac OS: como son Apple, implementan \u00fanicamente los programas que les da la gana. Hadoop: HDFS no es un sistema operativo, sino un sistema de archivos distribuido. Aunque algunos comandos de Hadoop tienen el mismo nombre, la forma de trabajar es totalmente diferente.","title":"L\u00ednea de comandos de Linux"},{"location":"Cosas%20varias/Linux/#linea-de-comandos-de-linux","text":"La mayor\u00eda de sistemas operativos basados en Linux contienen un conjunto de programas peque\u00f1os conocidos como GNU coreutils, que dan acceso a la funcionalidad b\u00e1sica del sistema: manejo de archivos, edici\u00f3n de texto, etc. Las opciones pueden ser cortas ( ls -l ) o largas ( ls --version ). Las opciones cortas se pueden dar todas a la vez: ls -lsr . Al contrario que Windows, Unix distingue may\u00fasculas de min\u00fasculas. Pulsar Ctrl-C interrumpe la ejecuci\u00f3n de cualquier programa.","title":"L\u00ednea de comandos de Linux"},{"location":"Cosas%20varias/Linux/#sistema-de-archivos","text":"Al igual que Windows, Unix emplea un sistema de archivos jer\u00e1rquico. Windows Linux Directorio ra\u00edz C:\\ / Otros discos duros / USB / etc. D:\\, E:\\, etc. /mnt/sda1, /mnt/sda2, etc. Directorio casa del usuario C:\\Usuarios\\nombre_usuario /home/ nombre_usuario Atajo: ~ Escritorio C:\\Usuarios\\nombre\\Escritorio ~/Desktop Archivos de programa C:\\Archivos de programa C:\\Archivos de programa (x64) /usr/bin El s\u00edmbolo ~ se puede escribir con AltGr + 4 y hace referencia al directorio \u201ccasa\u201d del usuario actual. Las rutas pueden ser absolutas (/usr/ejemplo.txt) relativas al directorio actual (ejemplo.txt) relativas al directorio casa del usuario (~/ejemplo.txt)","title":"Sistema de archivos"},{"location":"Cosas%20varias/Linux/#navegacion","text":"pwd (present working directory): mostrar la ruta completa del directorio actual ls directorio (list): muestra todos los archivos en el directorio dado. Si no le pasamos ning\u00fan argumento, utiliza el directorio actual ls -l incluye informaci\u00f3n sobre el tama\u00f1o y permisos de los archivos ls -a incluye archivos ocultos ls -R muestra tambi\u00e9n los archivos en subdirectorios cd directorio (change directory): se mueve al directorio indicado Para ir al directorio superior, cd ..","title":"Navegaci\u00f3n"},{"location":"Cosas%20varias/Linux/#archivos-y-directorios","text":"touch nombre : crea un archivo vac\u00edo mkdir directorio (make directory): crea un directorio vac\u00edo cp origen destino : copia el archivo origen al destino mv origen destino : mueve el archivo origen al destino rm archivo (remove): elimina un archivo rmdir elimina directorios Para borrar todos los archivos y subdirectorios, rm \u2013rf *.* (mirar bien en qu\u00e9 directorio estamos antes de ejecutar esto)","title":"Archivos y directorios"},{"location":"Cosas%20varias/Linux/#ayuda","text":"man comando (manual): muestra instrucciones de uso de un comando Los manuales suelen tener muchas p\u00e1ginas. Para salir, pulsar q","title":"Ayuda"},{"location":"Cosas%20varias/Linux/#manejo-de-texto","text":"La mayor\u00eda de programas en Linux utilizan texto plano como entrada y salida. cat archivo : abre un archivo de texto y muestra su contenido. Si pasamos varios archivos, los concatena y despu\u00e9s los muestra todos juntos grep texto archivo : busca el texto dado en un archivo sed archivo operaci\u00f3n (stream editor): realiza b\u00fasquedas y sustituciones sencillas. El primer argumento es el archivo, y el segundo es un string cuya primera letra indica la operaci\u00f3n que queremos hacer: s (substitute): busca una expresi\u00f3n y la sustituye por otra. La sintaxis es \u2018 s/ regexp / sustituci\u00f3n / g \u2019 donde regexp es la expresi\u00f3n regular que buscamos, sustituci\u00f3n es el texto con el que la reemplazamos y la g (opcional) indica que queremos reemplazar todas las veces que aparezca la expresi\u00f3n y no s\u00f3lo la primera. d (delete): borra el texto. Se suele usar en combinaci\u00f3n con un n\u00famero de l\u00ednea: 1d elimina la primera l\u00ednea. Normalmente la salida de un programa se muestra por pantalla. El operador | (pipe) permite concatenar comandos, pasando la salida de un comando como entrada al siguiente. Tambi\u00e9n podemos almacenar la salida del comando en un archivo: >> a\u00f1ade la informaci\u00f3n al final del archivo, y > lo sobreescribe. Por ejemplo, si tenemos una serie de precios que usan la coma como separador decimal y queremos cambiarlas a puntos:","title":"Manejo de texto"},{"location":"Cosas%20varias/Linux/#editores-de-texto","text":"Casi todas las distribuciones de Linux incluyen al menos uno de estos editores de texto gr\u00e1ficos, que se pueden usar directamente desde la consola: nano : editor sencillo. Para abrir un archivo escribir en la consola nano nombre_archivo , y al terminar de editarlo salir con Ctrl-X. vi : editor potente pero sorprendentemente dif\u00edcil de usar. Ver https://www.openvim.com/","title":"Editores de texto"},{"location":"Cosas%20varias/Linux/#secure-shell","text":"Normalmente vamos a querer utilizar estos comandos para controlar m\u00e1quinas de forma remota. La forma est\u00e1ndar de abrir un terminal en un ordenador diferente es ssh (secure shell). Antes de poder usarlo hay que instalar y configurar un servidor ssh en el ordenador remoto: apt install openssh-server . Una vez hecho esto, nos podemos conectar a \u00e9l desde cualquier otro ordenador con ssh *direcci\u00f3nIP* , y ejecutar comandos con ssh *servidor comando* . Los servicios en la nube (Google Cloud, etc.) ya manejan todo esto por nosotros, y permiten abrir una terminal en la m\u00e1quina remota pulsando un bot\u00f3n:","title":"Secure Shell"},{"location":"Cosas%20varias/Linux/#instalar-programas","text":"Las m\u00e1quinas virtuales suelen usar sistemas operativos con muy pocos programas instalados, para ahorrar espacio. Para instalar lo que necesitemos hay que usar un \u201cgestor de paquetes\u201d: En Debian, Ubuntu y similares, apt : apt get apt remove. Esto mantiene la configuraci\u00f3n del programa; para borrarla usar apt purge apt update En Alpine y otras distribuciones minimalistas, apk : apk add apk del apk update Estos comandos requieren permisos de superusuario. (Ver secci\u00f3n siguiente)","title":"Instalar programas"},{"location":"Cosas%20varias/Linux/#comandos-de-superusuario","text":"En Unix hay usuarios normales, con permisos restringidos, y superusuarios (tambi\u00e9n llamados root), que tienen control absoluto sobre el sistema. En general se usa siempre una cuenta de usuario normal, para evitar romper cosas por accidente. Si queremos usar un comando que requiere permisos de superusuario, podemos usar la utilidad sudo , que ejecutar\u00e1 solamente ese comando como root (despu\u00e9s de pedirnos la contrase\u00f1a).","title":"Comandos de superusuario"},{"location":"Cosas%20varias/Linux/#cron-jobs","text":"Cron se usa para que el sistema ejecute autom\u00e1ticamente scripts cada cierto tiempo. Cada usuario tiene una lista de trabajos cron, que se puede editar con crontab -e . En cada l\u00ednea de la lista hay cinco n\u00fameros, que representan el minuto, hora, d\u00eda, mes y d\u00eda de la semana en que se debe ejecutar el script, y se pueden reemplazar por un * para que se ejecute siempre. Por ejemplo, para ejecutar un comando todos los d\u00edas a las 9:24 de la noche, escribimos 24 21 * * * . A continuaci\u00f3n se escribe el comando a ejecutar. Los scripts se ejecutar\u00e1n con los permisos del usuario al que corresponde la cron file. Tambi\u00e9n hay una cron file del sistema, que puede ejecutar scripts con los permisos de cualquier usuario. Para que el script funcione hay que marcarlo como ejecutable: sudo chmod +x backup.sh .","title":"Cron jobs"},{"location":"Cosas%20varias/Linux/#alternativas","text":"Hay tambi\u00e9n sistemas basados en Linux que no utilizan las coreutils de GNU. La mayor\u00eda de los comandos en este documento seguir\u00e1n funcionando, pero algunos tendr\u00e1n menos opciones o directamente no existir\u00e1n. A veces podremos instalar los programas que faltan con un gestor de aplicaciones. Busybox: es un sustituto de las coreutils para sistemas que tienen que ocupar muy poco espacio, como Alpine Linux. Contiene la mayor\u00eda de los programas, pero suelen tener menos opciones. Mac OS: como son Apple, implementan \u00fanicamente los programas que les da la gana. Hadoop: HDFS no es un sistema operativo, sino un sistema de archivos distribuido. Aunque algunos comandos de Hadoop tienen el mismo nombre, la forma de trabajar es totalmente diferente.","title":"Alternativas"},{"location":"Cosas%20varias/git/","text":"Git y GitHub Instalaci\u00f3n Descargar git de https://git-scm.com/download, y seguir estos pasos: # Configurar el nombre y correo que aparecer\u00e1 en nuestros commits git config --global user.name \"nombre\" git config --global user.email \"correo@ejemplo.com\" # Abrir un cuadro de di\u00e1logo para a\u00f1adir nuestra cuenta de GitHub git config --global credential.helper manager-core La mayor\u00eda de editores de c\u00f3digo tienen alguna interfaz gr\u00e1fica para trabajar con git, que suele ser m\u00e1s c\u00f3moda que la l\u00ednea de comandos. Funcionamiento de git git maneja tres zonas de archivos: Los datos internos de git se almacenan en una subcarpeta oculta, .git . Todos los dem\u00e1s archivos forman parte del working directory. Para trabajar con git modificamos archivos en el working directory, seleccionamos los cambios que queremos guardar con git add , y finalmente los guardamos en el repositorio con git commit . Si tenemos un repositorio remoto, en GitHub o en otro proveedor, podemos sincronizar nuestro repositorio local y el remoto con git pull y git push . git no almacena las diferencias entre versiones, sino que guarda una copia completa de todos los archivos cada vez que cambian. Por este motivo no es aconsejable utilizarlo para archivos grandes; en ese caso usar Git Large File Storage . Crear un repositorio Para crear un repositorio vac\u00edo, git init . Para copiar un repositorio existente, git clone *enlace* . Guardar nuestros cambios Para guardar los archivos en el repositorio y poder compartirlos en GitHub hay que meterlos en un commit. git add . pasa todos los archivos a la staging area , una zona intermedia donde se pueden revisar antes de guardarlos definitivamente , y git commit los guarda en el repositorio. git add . # El punto selecciona todos los archivos git commit -m \"Mensaje\" # Guarda los archivos en el repositorio Ver el historial Para mostrar el historial de commits del proyecto, git log --all --oneline --graph . El contenido de git log puede ser muy largo; si no cabe en pantalla, pulsar Intro para avanzar y q para salir. Los commits est\u00e1n identificados por un c\u00f3digo en hexadecimal: 1926b1... . Para referirse a ellos basta con dar las cuatro o cinco primeras letras. HEAD representa el commit actual. git diff *commit1* *commit2* muestra las diferencias entre dos commits. Si hemos introducido un fallo pero no sabemos exactamente cu\u00e1ndo, usar git bisect *commit_malo* *commit_bueno* (donde el primer commit presenta el fallo y el segundo no). Volver a un commit anterior git checkout *commit* . (no olvidarse del punto) reemplaza nuestro working directory con el contenido del commit dado. Este comando borra de forma permanente cualquier cambio no guardado que hayamos hecho desde entonces. Etiquetas Las etiquetas se usan para marcar commits importantes, y aparecen resaltadas en git log y en GitHub. git tag *etiqueta* *commit* a\u00f1ade una etiqueta al commit indicado (o al commit actual si omitimos ese argumento). gitignore Podemos crear un archivo .gitignore en el directorio ra\u00edz, que indica los archivos y directorios que git va a ignorar. Es buena idea meter aqu\u00ed las configuraciones de la IDE o los resultados de compilaci\u00f3n (normalmente s\u00f3lo queremos compartir el c\u00f3digo fuente). Claves de API Si nuestra aplicaci\u00f3n contiene claves de API o contrase\u00f1as, utilizar GitHub secrets . Trabajo en equipo Git es un sistema distribuido: cada usuario tiene una copia independiente del repositorio entero. Los cambios que hacemos con git commit, branch, merge, etc. s\u00f3lo afectan a nuestro repositorio local, y no se sincronizan autom\u00e1ticamente con el repositorio remoto en GitHub; hay que hacerlo manualmente con git pull y git push . Enlazar el repositorio local con el remoto Por defecto llamamos \"origin\" al repositorio remoto: git remote add origin https://ejemplo.git S\u00f3lo hay que hacer esto una vez; y clonar un repositorio remoto ya lo hace por nosotros. Sincronizar git pull descarga los cambios en el repositorio remoto y los incorpora a nuestro proyecto. Si alguien ha estado modificando archivos al mismo tiempo que nosotros, probablemente se producir\u00e1 un merge conflict , que nos indica que git tiene dos versiones del mismo archivo y no sabe con cu\u00e1l debe quedarse. Para resolver el conflicto hay que elegir una de las versiones (o una mezcla de ambas) y crear un nuevo commit. La mayor\u00eda de IDEs tienen una opci\u00f3n dedicada a resolver estos conflictos: Para subir nuestros cambios al repositorio remoto, usar git push origin master . Resumen \u00c9sta es probablemente la forma m\u00e1s sencilla de usar git y GitHub para un equipo peque\u00f1o: Antes de empezar a trabajar, descargar los cambios que han hecho otros compa\u00f1eros con git pull Al terminar de trabajar: git add . git commit Hacer otro git pull , por si alguien ha estado cambiando cosas al mismo tiempo que nosotros Si se produce alg\u00fan merge conflict, corregirlo y volver al paso 1 Si no hay conflictos, enviar nuestros cambios al repositorio remoto con git push En proyectos m\u00e1s grandes, tradicionalmente se usaban flujos de trabajo m\u00e1s complejos como GitFlow . En la actualidad es m\u00e1s com\u00fan usar un modelo con una sola rama, a la que se van a\u00f1adiendo cambios peque\u00f1os de forma tan frecuente como sea posible ( trunk-based development ). Git en Mac Para que Git maneje los caracteres de final de l\u00ednea y acentos correctamente, hay que activar estas dos opciones: git config --global core.autocrlf input git config --global core.precomposeunicode true Adem\u00e1s, para usar GitHub hay que utilizar un token, ya que la autenticaci\u00f3n por contrase\u00f1a o a trav\u00e9s de pasarela web no funciona.","title":"Git y GitHub"},{"location":"Cosas%20varias/git/#git-y-github","text":"","title":"Git y GitHub"},{"location":"Cosas%20varias/git/#instalacion","text":"Descargar git de https://git-scm.com/download, y seguir estos pasos: # Configurar el nombre y correo que aparecer\u00e1 en nuestros commits git config --global user.name \"nombre\" git config --global user.email \"correo@ejemplo.com\" # Abrir un cuadro de di\u00e1logo para a\u00f1adir nuestra cuenta de GitHub git config --global credential.helper manager-core La mayor\u00eda de editores de c\u00f3digo tienen alguna interfaz gr\u00e1fica para trabajar con git, que suele ser m\u00e1s c\u00f3moda que la l\u00ednea de comandos.","title":"Instalaci\u00f3n"},{"location":"Cosas%20varias/git/#funcionamiento-de-git","text":"git maneja tres zonas de archivos: Los datos internos de git se almacenan en una subcarpeta oculta, .git . Todos los dem\u00e1s archivos forman parte del working directory. Para trabajar con git modificamos archivos en el working directory, seleccionamos los cambios que queremos guardar con git add , y finalmente los guardamos en el repositorio con git commit . Si tenemos un repositorio remoto, en GitHub o en otro proveedor, podemos sincronizar nuestro repositorio local y el remoto con git pull y git push . git no almacena las diferencias entre versiones, sino que guarda una copia completa de todos los archivos cada vez que cambian. Por este motivo no es aconsejable utilizarlo para archivos grandes; en ese caso usar Git Large File Storage .","title":"Funcionamiento de git"},{"location":"Cosas%20varias/git/#crear-un-repositorio","text":"Para crear un repositorio vac\u00edo, git init . Para copiar un repositorio existente, git clone *enlace* .","title":"Crear un repositorio"},{"location":"Cosas%20varias/git/#guardar-nuestros-cambios","text":"Para guardar los archivos en el repositorio y poder compartirlos en GitHub hay que meterlos en un commit. git add . pasa todos los archivos a la staging area , una zona intermedia donde se pueden revisar antes de guardarlos definitivamente , y git commit los guarda en el repositorio. git add . # El punto selecciona todos los archivos git commit -m \"Mensaje\" # Guarda los archivos en el repositorio","title":"Guardar nuestros cambios"},{"location":"Cosas%20varias/git/#ver-el-historial","text":"Para mostrar el historial de commits del proyecto, git log --all --oneline --graph . El contenido de git log puede ser muy largo; si no cabe en pantalla, pulsar Intro para avanzar y q para salir. Los commits est\u00e1n identificados por un c\u00f3digo en hexadecimal: 1926b1... . Para referirse a ellos basta con dar las cuatro o cinco primeras letras. HEAD representa el commit actual. git diff *commit1* *commit2* muestra las diferencias entre dos commits. Si hemos introducido un fallo pero no sabemos exactamente cu\u00e1ndo, usar git bisect *commit_malo* *commit_bueno* (donde el primer commit presenta el fallo y el segundo no).","title":"Ver el historial"},{"location":"Cosas%20varias/git/#volver-a-un-commit-anterior","text":"git checkout *commit* . (no olvidarse del punto) reemplaza nuestro working directory con el contenido del commit dado. Este comando borra de forma permanente cualquier cambio no guardado que hayamos hecho desde entonces.","title":"Volver a un commit anterior"},{"location":"Cosas%20varias/git/#etiquetas","text":"Las etiquetas se usan para marcar commits importantes, y aparecen resaltadas en git log y en GitHub. git tag *etiqueta* *commit* a\u00f1ade una etiqueta al commit indicado (o al commit actual si omitimos ese argumento).","title":"Etiquetas"},{"location":"Cosas%20varias/git/#gitignore","text":"Podemos crear un archivo .gitignore en el directorio ra\u00edz, que indica los archivos y directorios que git va a ignorar. Es buena idea meter aqu\u00ed las configuraciones de la IDE o los resultados de compilaci\u00f3n (normalmente s\u00f3lo queremos compartir el c\u00f3digo fuente).","title":"gitignore"},{"location":"Cosas%20varias/git/#claves-de-api","text":"Si nuestra aplicaci\u00f3n contiene claves de API o contrase\u00f1as, utilizar GitHub secrets .","title":"Claves de API"},{"location":"Cosas%20varias/git/#trabajo-en-equipo","text":"Git es un sistema distribuido: cada usuario tiene una copia independiente del repositorio entero. Los cambios que hacemos con git commit, branch, merge, etc. s\u00f3lo afectan a nuestro repositorio local, y no se sincronizan autom\u00e1ticamente con el repositorio remoto en GitHub; hay que hacerlo manualmente con git pull y git push .","title":"Trabajo en equipo"},{"location":"Cosas%20varias/git/#enlazar-el-repositorio-local-con-el-remoto","text":"Por defecto llamamos \"origin\" al repositorio remoto: git remote add origin https://ejemplo.git S\u00f3lo hay que hacer esto una vez; y clonar un repositorio remoto ya lo hace por nosotros.","title":"Enlazar el repositorio local con el remoto"},{"location":"Cosas%20varias/git/#sincronizar","text":"git pull descarga los cambios en el repositorio remoto y los incorpora a nuestro proyecto. Si alguien ha estado modificando archivos al mismo tiempo que nosotros, probablemente se producir\u00e1 un merge conflict , que nos indica que git tiene dos versiones del mismo archivo y no sabe con cu\u00e1l debe quedarse. Para resolver el conflicto hay que elegir una de las versiones (o una mezcla de ambas) y crear un nuevo commit. La mayor\u00eda de IDEs tienen una opci\u00f3n dedicada a resolver estos conflictos: Para subir nuestros cambios al repositorio remoto, usar git push origin master .","title":"Sincronizar"},{"location":"Cosas%20varias/git/#resumen","text":"\u00c9sta es probablemente la forma m\u00e1s sencilla de usar git y GitHub para un equipo peque\u00f1o: Antes de empezar a trabajar, descargar los cambios que han hecho otros compa\u00f1eros con git pull Al terminar de trabajar: git add . git commit Hacer otro git pull , por si alguien ha estado cambiando cosas al mismo tiempo que nosotros Si se produce alg\u00fan merge conflict, corregirlo y volver al paso 1 Si no hay conflictos, enviar nuestros cambios al repositorio remoto con git push En proyectos m\u00e1s grandes, tradicionalmente se usaban flujos de trabajo m\u00e1s complejos como GitFlow . En la actualidad es m\u00e1s com\u00fan usar un modelo con una sola rama, a la que se van a\u00f1adiendo cambios peque\u00f1os de forma tan frecuente como sea posible ( trunk-based development ).","title":"Resumen"},{"location":"Cosas%20varias/git/#git-en-mac","text":"Para que Git maneje los caracteres de final de l\u00ednea y acentos correctamente, hay que activar estas dos opciones: git config --global core.autocrlf input git config --global core.precomposeunicode true Adem\u00e1s, para usar GitHub hay que utilizar un token, ya que la autenticaci\u00f3n por contrase\u00f1a o a trav\u00e9s de pasarela web no funciona.","title":"Git en Mac"},{"location":"Estad%C3%ADstica/R/","text":"R R es un lenguaje de programaci\u00f3n orientado a la estad\u00edstica. Recursos Ejercicios en clase : gasolineras , primer ejercicio de clase , examen parcial , \u00faltimo ejercicio de clase \"Cheat sheets\": en el men\u00fa de RStudio, Help -> Cheat Sheets Documentaci\u00f3n del tidyverse Libros: R for Data Science , Advanced R Introducci\u00f3n Variables Asignamos variables con x <- 3 (atajo en RStudio: Alt + - ). Los nombres de variables y funciones se suelen escribir en min\u00fasculas separadas por barras bajas. Las may\u00fasculas y min\u00fasculas se consideran letras diferentes. Los tipos m\u00e1s usados son: character : a pesar del nombre, pueden tener varias letras enteros ( integer ), punto flotante ( double ) logical : TRUE o FALSE list : listas ( list(1, 2, 3) ) o diccionarios ( list(a=1, b=2) ) vectores (similares a las listas, pero s\u00f3lo contienen valores de un tipo): c(1, 2, 3) ; 1:5 raw : datos en formato binario date para fechas, dttm (datetime) para fechas con d\u00eda y hora Los missing values aparecen representados como NA . La funci\u00f3n typeof muestra el tipo de una variable. Para cambiar de tipo usar as.integer , as.tibble , etc. Para acceder a valores en una lista o vector, usar corchetes: vector[3], lista[2], diccionario[\"a\"] . Al contrario que en Python, los \u00edndices empiezan en 1. RStudio Ctrl + Intro ejecuta la l\u00ednea actual, y Ctrl + Shift + S ejecuta el script entero. Ctrl + Shift + R a\u00f1ade una secci\u00f3n nueva. RStudio conserva el estado del programa y los paquetes instalados al cerrarlo y volverlo a abrir. El icono de la escoba en el panel Environment borra los objetos en el entorno. Para reiniciar por completo el entorno (borrar los objetos y eliminar los paquetes instalados), Ctrl + Shift + F10 . R busca archivos en el \"working directory\", que podemos cambiar con las opciones en el panel Files: Pipes El operador pipe ( %>% , atajo en RStudio: Ctrl + Shift + M ) pasa la variable a su izquierda como el primer argumento para la funci\u00f3n a su derecha: x %>% f(y, z) equivale a f(x, y, z) . Lo usaremos para encadenar funciones sin tener que almacenar los resultados intermedios en variables. Funciones suma <- function ( x , y ){ x + y } Bucles for ( url in url_vector ) { res <- GET ( url ) %>% headers () peso <- peso + as.numeric ( res_ $ `content-length` ) } Condicionales mutate ( flights , distancia = ifelse ( distance > 5000 , \"largo\" , \"corto\" )) mutate ( ccaa = case_when ( idccaa == \"01\" ~ \"Andaluc\u00eda\" , idccaa == \"02\" ~ \"Arag\u00f3n\" )) Ejemplos En los ejemplos voy a usar el dataset flights , que se instala con pacman::p_load(nycflights13) . Tibbles Las tibbles son un formato de tablas usado en todo el tidyverse. Se pueden crear manualmente: tibble(x=1:10, y=2*x, z= 5) , pero normalmente las obtendremos a partir de un dataset. Para seleccionar una columna de la tibble usar el s\u00edmbolo $ : flights$carrier , o bien flights[[carrier]] . Si queremos varias columnas podemos usar dplyr::select(flights, dep_delay, arr_delay) , que devuelve otra tibble con s\u00f3lo las columnas elegidas. glimpse() imprime un resumen del contenido de la tibble. Las funciones del tidyverse usan tibbles, pero otras funciones de R siguen usando dataframes; las podemos convertir a tibbles con as_tibble(df) . Instalaci\u00f3n de paquetes Para ahorrarse l\u00edos utilizar el paquete pacman : packages.install ( \"pacman\" ) # Instalar pacman, s\u00f3lo hay que hacerlo una vez pacman :: p_load ( tidyverse , leaflet , janitor , httr , jsonlite , xml2 ) Core tidyverse El \"n\u00facleo\" de tidyverse son estos ocho paquetes (en negrita los que hemos usado en el curso): dplyr : manipulaci\u00f3n de datos ( mutate , select , filter ...) tidyr : limpieza de datos ( drop_na , replace_na ...) readr : lectura de CSV ( read_csv ). Para leer otros formatos hay que usar otros paquetes (ver abajo) tibble : la estructura b\u00e1sica de datos del tidyverse ggplot2 : gr\u00e1ficas. Seg\u00fan dicen ellos, \"It\u2019s hard to succinctly describe how ggplot2 works because it embodies a deep philosophy of visualisation\" . En la pr\u00e1ctica, no hay forma humana de entenderlo. Aqu\u00ed hay un tutorial purrr stringr : manipulaci\u00f3n de strings ( str_extract ...) forcats Adem\u00e1s hay muchos paquetes m\u00e1s peque\u00f1os, que dan funcionalidad adicional: httr , jsonlite , xml2 , readxl ... Instalar el tidyverse instala todos estos paquetes, pero p_load(tidyverse) s\u00f3lo carga los ocho paquetes del n\u00facleo. Para usar los otros paquetes hay que a\u00f1adir el nombre del paquete y dos puntos ( jsonlite::fromJSON() ), o bien cargar el paquete manualmente ( pacman::p_load(jsonlite) , despu\u00e9s ya se puede escribir fromJSON() ). Tambi\u00e9n hemos usado algunos paquetes que no son parte del tidyverse: janitor : limpieza de datos leaflet : dibuja mapas Lectura de datos csv Utilizar readr::read_csv : el primer par\u00e1metro es el nombre de archivo (relativo al working directory) si el archivo est\u00e1 separado por ; en vez de , , utilizar read_csv2 read_csv asume que la primera fila contiene los nombres de las columnas. Si esta fila no existe, col_names=FALSE para ignorar las n primeras filas, skip=n si nuestros n\u00fameros tienen separadores diferentes de los que se usan en EE.UU., indicarlos con locale=locale(decimal_mark=\",\", grouping_mark=\".\") R intenta inferir el tipo de cada columna, generalmente sin mucho \u00e9xito. Si no funciona bien, limpiar los datos manualmente y ejecutar type_convert(tibble) para que vuelva a inferir los tipos. write_csv pasa la tibble a un archivo. Otros formatos Excel: readxl::read_excel XML: xml2::read_xml(url, encoding=\"UTF-8\") , write_xml(archivo) JSON: jsonlite::fromJSON(url) , toJSON(archivo) Acceso a Internet Usar httr::GET(url) . Esto nos da un objeto response , que contiene la informaci\u00f3n (\"content\") y metadatos. r <- httr :: GET ( url ) content ( r ) # Contenido status_code ( r ) # C\u00f3digo de estado headers ( r ) # Metadatos de la respuesta Podemos pasar el contenido directamente al disco duro con httr::GET(url, write_disk(\"archivo.xml\")) . Los c\u00f3digos de estado m\u00e1s comunes son: 200 OK 204 No Content: petici\u00f3n correcta, pero la respuesta del servidor est\u00e1 vac\u00eda 400 Bad Request: petici\u00f3n inv\u00e1lida 401 Unauthenticated / 403 Forbidden: no tenemos permiso para acceder 404 Not Found: no hay ning\u00fan recurso en la direcci\u00f3n solicitada 5xx : errores del servidor Si no lee bien el contenido, utilizar content(r, \"text\", encoding=\"UTF=8\") (el encoding que hay que usar est\u00e1 en los headers) \u00f3 content(r, \"raw\") . url <- \"https://sedeaplicaciones.minetur.gob.es/ServiciosRESTCarburantes/PreciosCarburantes/EstacionesTerrestres/\" r <- httr :: GET ( url ) # Si quisi\u00e9ramos escribirlo en disco, GET(url, write_disk(\"gasolineras.xml\")) status_code ( r ) # 200 (OK) # Consultar el header content-type para ver el tipo de archivo y el encoding headers ( r ) # content-type: application/xml; charset=utf-8 # Para leer un XML, pasarle el contenido del request como texto: datos_xml <- xml2 :: read_xml ( content ( r , as = \"text\" )) # Para leer un JSON, podemos pasar directamente la URL (sin necesidad de usar httr) datos_json <- jsonlite :: fromJSON ( url ) Tambi\u00e9n podemos enviar peticiones POST con httr::POST . Para subir un archivo, httr :: POST ( url , body = list ( x = upload_file ( 'ejemplo.csv' ))) Limpieza de datos El primer paso es usar estas funciones de janitor ( janitor no es parte del tidyverse y hay que instalarlo por separado): clean_names : elimina caracteres no v\u00e1lidos de los nombres de variables remove_empty : elimina filas y columnas que solamente contienen NA Despu\u00e9s, usar glimpse() para ver un listado de columnas y comprobar que ha inferido los tipos correctamente. En caso contrario, corregirlos con la opci\u00f3n across de dplyr::mutate : flights <- flights %>% mutate(across(year, as.character)) Manipulaci\u00f3n de datos (dplyr) Las funciones en dplyr s\u00f3lo funcionan bien con datos \"ordenados\" (forma rectangular, una observaci\u00f3n por fila, una variable por columna, missing values indicados como NA ). Siempre devuelven una tibble nueva, sin modificar la original. Las funciones b\u00e1sicas son (en negrita las que hemos usado): select : elegir solamente algunas variables (columnas) select ( flights , month ) select ( flights , carrier , arr_delay ) select ( flights , contains ( \"delay\" )) select tambi\u00e9n se puede usar para cambiar el orden de las columnas: # Mover los aeropuertos de origen y destino al principio select ( flights , origin , dest , everything ()) rename : renombrar columnas. Poner el nombre nuevo delante del = , y el antiguo detr\u00e1s: rename ( flights , distancia = distance ) filter : eliminar las observaciones que no cumplen las condiciones dadas. Tambi\u00e9n elimina las observaciones con missing values filter ( flights , month == 5 ) filter ( flights , month == 5 , day == 12 ) filter ( flights , month == 11 | month == 12 ) filter ( flights , month %in% c ( 11 , 12 )) mutate : a\u00f1adir columnas nuevas al final de la tibble, cuyos valores pueden depender de los valores en otras columnas mutate ( flights , delayed = ( arr_delay > 0 )) mutate ( flights , distancia = ifelse ( distance > 5000 , \"largo\" , \"corto\" )) mutate ( ccaa = case_when ( idccaa == \"01\" ~ \"Andaluc\u00eda\" , idccaa == \"02\" ~ \"Arag\u00f3n\" )) arrange : reordenar las filas en funci\u00f3n de los valores de una o varias columnas. Las filas con missing values se colocan al final # Ordenar los vuelos por fecha arrange ( flights , year , month , day ) # Ordenar los vuelos por su retraso, en orden descendente arrange ( flights , desc ( arr_delay )) count : cuenta el n\u00famero de valores \u00fanicos de una columna, y nos da el n\u00famero de veces que aparece cada uno. count ( flights , carrier ) count ( flights , carrier , sort = T ) separate : divide una columna en dos o m\u00e1s columnas nuevas. Los argumentos son la columna de origen, los nombres de las columnas de destino, y el separador separate ( flights , col = time_hour , into = c ( 'date' , 'time' ), sep = \" \" ) Estas funciones se pueden usar en combinaci\u00f3n con group_by() , que agrupa los datos que tienen propiedades iguales (por ejemplo, los vuelos de la misma compa\u00f1\u00eda), pero hasta que lo veamos en clase no me voy a meter en ello. M\u00e1s informaci\u00f3n aqu\u00ed . Gr\u00e1ficos Usamos el paquete ggplot2 . Se utiliza llamando a la funci\u00f3n ggplot , donde el par\u00e1metro data es el dataset y aes la columna, y sum\u00e1ndole otra funci\u00f3n para dar una forma al gr\u00e1fico: # Histograma (var. continuas) ggplot ( data = flights , aes ( arr_delay )) + geom_histogram () # Diagrama de barras (var. discretas) ggplot ( data = flights , aes ( carrier )) + geom_bar () # Scatterplot (dos variables) ggplot ( data = flights , aes ( dep_delay , arr_delay )) + geom_point () Mapas Utilizamos los paquetes leaflet y leaflet.extras (asegurarse de cargar los dos). Por defecto utiliza el mapa OSM (OpenStreetMap), que incluye informaci\u00f3n de carreteras, etc. Para visualizaci\u00f3n es mejor usar un mapa m\u00e1s limpio, como CartoDB. Para incluir nuestros datos en el mapa usar addMarkers, addCircles \u00f3 addHeatmap. leaflet va a buscar la informaci\u00f3n de latitud y longitud en columnas que se llamen latitude y longitude ; si las nuestras tienen un nombre diferente, pasarlas como argumentos con la virgulilla (~). m <- df3 %>% leaflet () %>% addProviderTiles ( \"CartoDB\" ) %>% # Add default OpenStreetMap map tiles addHeatmap ( lng =~ `Longitud (WGS84)` , lat =~ Latitud , radius = 10 ) # addCircles(lng=~`Longitud (WGS84)`, lat=~Latitud, popup=\"Gasolinera\") m # Mostrar el mapa Para m\u00e1s opciones de visualizaci\u00f3n, https://www.htmlwidgets.org . Uso de memoria Podemos ver cu\u00e1nta memoria ocupa un objeto de R con object.size(objeto) . Para ver cu\u00e1nto ocupan los datos descargados de Internet con httr , consultar la propiedad content_length en su header. Los resultados vienen en bytes. Manipulaci\u00f3n de strings Utilizamos el paquete stringr , incluido en el core tidyverse. La funci\u00f3n str_extract permite buscar texto con expresiones regulares: shopping_list <- c ( \"apples x4\" , \"bag of flour\" , \"bag of sugar\" , \"milk x2\" ) str_extract ( shopping_list , \"\\\\d\" ) Si hay varios substrings que cumplan la expresi\u00f3n regular, s\u00f3lo nos devolver\u00e1 el primero. Para obtenerlos todos, usar str_extract_all .","title":"R"},{"location":"Estad%C3%ADstica/R/#r","text":"R es un lenguaje de programaci\u00f3n orientado a la estad\u00edstica.","title":"R"},{"location":"Estad%C3%ADstica/R/#recursos","text":"Ejercicios en clase : gasolineras , primer ejercicio de clase , examen parcial , \u00faltimo ejercicio de clase \"Cheat sheets\": en el men\u00fa de RStudio, Help -> Cheat Sheets Documentaci\u00f3n del tidyverse Libros: R for Data Science , Advanced R","title":"Recursos"},{"location":"Estad%C3%ADstica/R/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"Estad%C3%ADstica/R/#variables","text":"Asignamos variables con x <- 3 (atajo en RStudio: Alt + - ). Los nombres de variables y funciones se suelen escribir en min\u00fasculas separadas por barras bajas. Las may\u00fasculas y min\u00fasculas se consideran letras diferentes. Los tipos m\u00e1s usados son: character : a pesar del nombre, pueden tener varias letras enteros ( integer ), punto flotante ( double ) logical : TRUE o FALSE list : listas ( list(1, 2, 3) ) o diccionarios ( list(a=1, b=2) ) vectores (similares a las listas, pero s\u00f3lo contienen valores de un tipo): c(1, 2, 3) ; 1:5 raw : datos en formato binario date para fechas, dttm (datetime) para fechas con d\u00eda y hora Los missing values aparecen representados como NA . La funci\u00f3n typeof muestra el tipo de una variable. Para cambiar de tipo usar as.integer , as.tibble , etc. Para acceder a valores en una lista o vector, usar corchetes: vector[3], lista[2], diccionario[\"a\"] . Al contrario que en Python, los \u00edndices empiezan en 1.","title":"Variables"},{"location":"Estad%C3%ADstica/R/#rstudio","text":"Ctrl + Intro ejecuta la l\u00ednea actual, y Ctrl + Shift + S ejecuta el script entero. Ctrl + Shift + R a\u00f1ade una secci\u00f3n nueva. RStudio conserva el estado del programa y los paquetes instalados al cerrarlo y volverlo a abrir. El icono de la escoba en el panel Environment borra los objetos en el entorno. Para reiniciar por completo el entorno (borrar los objetos y eliminar los paquetes instalados), Ctrl + Shift + F10 . R busca archivos en el \"working directory\", que podemos cambiar con las opciones en el panel Files:","title":"RStudio"},{"location":"Estad%C3%ADstica/R/#pipes","text":"El operador pipe ( %>% , atajo en RStudio: Ctrl + Shift + M ) pasa la variable a su izquierda como el primer argumento para la funci\u00f3n a su derecha: x %>% f(y, z) equivale a f(x, y, z) . Lo usaremos para encadenar funciones sin tener que almacenar los resultados intermedios en variables.","title":"Pipes"},{"location":"Estad%C3%ADstica/R/#funciones","text":"suma <- function ( x , y ){ x + y }","title":"Funciones"},{"location":"Estad%C3%ADstica/R/#bucles","text":"for ( url in url_vector ) { res <- GET ( url ) %>% headers () peso <- peso + as.numeric ( res_ $ `content-length` ) }","title":"Bucles"},{"location":"Estad%C3%ADstica/R/#condicionales","text":"mutate ( flights , distancia = ifelse ( distance > 5000 , \"largo\" , \"corto\" )) mutate ( ccaa = case_when ( idccaa == \"01\" ~ \"Andaluc\u00eda\" , idccaa == \"02\" ~ \"Arag\u00f3n\" ))","title":"Condicionales"},{"location":"Estad%C3%ADstica/R/#ejemplos","text":"En los ejemplos voy a usar el dataset flights , que se instala con pacman::p_load(nycflights13) .","title":"Ejemplos"},{"location":"Estad%C3%ADstica/R/#tibbles","text":"Las tibbles son un formato de tablas usado en todo el tidyverse. Se pueden crear manualmente: tibble(x=1:10, y=2*x, z= 5) , pero normalmente las obtendremos a partir de un dataset. Para seleccionar una columna de la tibble usar el s\u00edmbolo $ : flights$carrier , o bien flights[[carrier]] . Si queremos varias columnas podemos usar dplyr::select(flights, dep_delay, arr_delay) , que devuelve otra tibble con s\u00f3lo las columnas elegidas. glimpse() imprime un resumen del contenido de la tibble. Las funciones del tidyverse usan tibbles, pero otras funciones de R siguen usando dataframes; las podemos convertir a tibbles con as_tibble(df) .","title":"Tibbles"},{"location":"Estad%C3%ADstica/R/#instalacion-de-paquetes","text":"Para ahorrarse l\u00edos utilizar el paquete pacman : packages.install ( \"pacman\" ) # Instalar pacman, s\u00f3lo hay que hacerlo una vez pacman :: p_load ( tidyverse , leaflet , janitor , httr , jsonlite , xml2 )","title":"Instalaci\u00f3n de paquetes"},{"location":"Estad%C3%ADstica/R/#core-tidyverse","text":"El \"n\u00facleo\" de tidyverse son estos ocho paquetes (en negrita los que hemos usado en el curso): dplyr : manipulaci\u00f3n de datos ( mutate , select , filter ...) tidyr : limpieza de datos ( drop_na , replace_na ...) readr : lectura de CSV ( read_csv ). Para leer otros formatos hay que usar otros paquetes (ver abajo) tibble : la estructura b\u00e1sica de datos del tidyverse ggplot2 : gr\u00e1ficas. Seg\u00fan dicen ellos, \"It\u2019s hard to succinctly describe how ggplot2 works because it embodies a deep philosophy of visualisation\" . En la pr\u00e1ctica, no hay forma humana de entenderlo. Aqu\u00ed hay un tutorial purrr stringr : manipulaci\u00f3n de strings ( str_extract ...) forcats Adem\u00e1s hay muchos paquetes m\u00e1s peque\u00f1os, que dan funcionalidad adicional: httr , jsonlite , xml2 , readxl ... Instalar el tidyverse instala todos estos paquetes, pero p_load(tidyverse) s\u00f3lo carga los ocho paquetes del n\u00facleo. Para usar los otros paquetes hay que a\u00f1adir el nombre del paquete y dos puntos ( jsonlite::fromJSON() ), o bien cargar el paquete manualmente ( pacman::p_load(jsonlite) , despu\u00e9s ya se puede escribir fromJSON() ). Tambi\u00e9n hemos usado algunos paquetes que no son parte del tidyverse: janitor : limpieza de datos leaflet : dibuja mapas","title":"Core tidyverse"},{"location":"Estad%C3%ADstica/R/#lectura-de-datos","text":"","title":"Lectura de datos"},{"location":"Estad%C3%ADstica/R/#csv","text":"Utilizar readr::read_csv : el primer par\u00e1metro es el nombre de archivo (relativo al working directory) si el archivo est\u00e1 separado por ; en vez de , , utilizar read_csv2 read_csv asume que la primera fila contiene los nombres de las columnas. Si esta fila no existe, col_names=FALSE para ignorar las n primeras filas, skip=n si nuestros n\u00fameros tienen separadores diferentes de los que se usan en EE.UU., indicarlos con locale=locale(decimal_mark=\",\", grouping_mark=\".\") R intenta inferir el tipo de cada columna, generalmente sin mucho \u00e9xito. Si no funciona bien, limpiar los datos manualmente y ejecutar type_convert(tibble) para que vuelva a inferir los tipos. write_csv pasa la tibble a un archivo.","title":"csv"},{"location":"Estad%C3%ADstica/R/#otros-formatos","text":"Excel: readxl::read_excel XML: xml2::read_xml(url, encoding=\"UTF-8\") , write_xml(archivo) JSON: jsonlite::fromJSON(url) , toJSON(archivo)","title":"Otros formatos"},{"location":"Estad%C3%ADstica/R/#acceso-a-internet","text":"Usar httr::GET(url) . Esto nos da un objeto response , que contiene la informaci\u00f3n (\"content\") y metadatos. r <- httr :: GET ( url ) content ( r ) # Contenido status_code ( r ) # C\u00f3digo de estado headers ( r ) # Metadatos de la respuesta Podemos pasar el contenido directamente al disco duro con httr::GET(url, write_disk(\"archivo.xml\")) . Los c\u00f3digos de estado m\u00e1s comunes son: 200 OK 204 No Content: petici\u00f3n correcta, pero la respuesta del servidor est\u00e1 vac\u00eda 400 Bad Request: petici\u00f3n inv\u00e1lida 401 Unauthenticated / 403 Forbidden: no tenemos permiso para acceder 404 Not Found: no hay ning\u00fan recurso en la direcci\u00f3n solicitada 5xx : errores del servidor Si no lee bien el contenido, utilizar content(r, \"text\", encoding=\"UTF=8\") (el encoding que hay que usar est\u00e1 en los headers) \u00f3 content(r, \"raw\") . url <- \"https://sedeaplicaciones.minetur.gob.es/ServiciosRESTCarburantes/PreciosCarburantes/EstacionesTerrestres/\" r <- httr :: GET ( url ) # Si quisi\u00e9ramos escribirlo en disco, GET(url, write_disk(\"gasolineras.xml\")) status_code ( r ) # 200 (OK) # Consultar el header content-type para ver el tipo de archivo y el encoding headers ( r ) # content-type: application/xml; charset=utf-8 # Para leer un XML, pasarle el contenido del request como texto: datos_xml <- xml2 :: read_xml ( content ( r , as = \"text\" )) # Para leer un JSON, podemos pasar directamente la URL (sin necesidad de usar httr) datos_json <- jsonlite :: fromJSON ( url ) Tambi\u00e9n podemos enviar peticiones POST con httr::POST . Para subir un archivo, httr :: POST ( url , body = list ( x = upload_file ( 'ejemplo.csv' )))","title":"Acceso a Internet"},{"location":"Estad%C3%ADstica/R/#limpieza-de-datos","text":"El primer paso es usar estas funciones de janitor ( janitor no es parte del tidyverse y hay que instalarlo por separado): clean_names : elimina caracteres no v\u00e1lidos de los nombres de variables remove_empty : elimina filas y columnas que solamente contienen NA Despu\u00e9s, usar glimpse() para ver un listado de columnas y comprobar que ha inferido los tipos correctamente. En caso contrario, corregirlos con la opci\u00f3n across de dplyr::mutate : flights <- flights %>% mutate(across(year, as.character))","title":"Limpieza de datos"},{"location":"Estad%C3%ADstica/R/#manipulacion-de-datos-dplyr","text":"Las funciones en dplyr s\u00f3lo funcionan bien con datos \"ordenados\" (forma rectangular, una observaci\u00f3n por fila, una variable por columna, missing values indicados como NA ). Siempre devuelven una tibble nueva, sin modificar la original. Las funciones b\u00e1sicas son (en negrita las que hemos usado): select : elegir solamente algunas variables (columnas) select ( flights , month ) select ( flights , carrier , arr_delay ) select ( flights , contains ( \"delay\" )) select tambi\u00e9n se puede usar para cambiar el orden de las columnas: # Mover los aeropuertos de origen y destino al principio select ( flights , origin , dest , everything ()) rename : renombrar columnas. Poner el nombre nuevo delante del = , y el antiguo detr\u00e1s: rename ( flights , distancia = distance ) filter : eliminar las observaciones que no cumplen las condiciones dadas. Tambi\u00e9n elimina las observaciones con missing values filter ( flights , month == 5 ) filter ( flights , month == 5 , day == 12 ) filter ( flights , month == 11 | month == 12 ) filter ( flights , month %in% c ( 11 , 12 )) mutate : a\u00f1adir columnas nuevas al final de la tibble, cuyos valores pueden depender de los valores en otras columnas mutate ( flights , delayed = ( arr_delay > 0 )) mutate ( flights , distancia = ifelse ( distance > 5000 , \"largo\" , \"corto\" )) mutate ( ccaa = case_when ( idccaa == \"01\" ~ \"Andaluc\u00eda\" , idccaa == \"02\" ~ \"Arag\u00f3n\" )) arrange : reordenar las filas en funci\u00f3n de los valores de una o varias columnas. Las filas con missing values se colocan al final # Ordenar los vuelos por fecha arrange ( flights , year , month , day ) # Ordenar los vuelos por su retraso, en orden descendente arrange ( flights , desc ( arr_delay )) count : cuenta el n\u00famero de valores \u00fanicos de una columna, y nos da el n\u00famero de veces que aparece cada uno. count ( flights , carrier ) count ( flights , carrier , sort = T ) separate : divide una columna en dos o m\u00e1s columnas nuevas. Los argumentos son la columna de origen, los nombres de las columnas de destino, y el separador separate ( flights , col = time_hour , into = c ( 'date' , 'time' ), sep = \" \" ) Estas funciones se pueden usar en combinaci\u00f3n con group_by() , que agrupa los datos que tienen propiedades iguales (por ejemplo, los vuelos de la misma compa\u00f1\u00eda), pero hasta que lo veamos en clase no me voy a meter en ello. M\u00e1s informaci\u00f3n aqu\u00ed .","title":"Manipulaci\u00f3n de datos (dplyr)"},{"location":"Estad%C3%ADstica/R/#graficos","text":"Usamos el paquete ggplot2 . Se utiliza llamando a la funci\u00f3n ggplot , donde el par\u00e1metro data es el dataset y aes la columna, y sum\u00e1ndole otra funci\u00f3n para dar una forma al gr\u00e1fico: # Histograma (var. continuas) ggplot ( data = flights , aes ( arr_delay )) + geom_histogram () # Diagrama de barras (var. discretas) ggplot ( data = flights , aes ( carrier )) + geom_bar () # Scatterplot (dos variables) ggplot ( data = flights , aes ( dep_delay , arr_delay )) + geom_point ()","title":"Gr\u00e1ficos"},{"location":"Estad%C3%ADstica/R/#mapas","text":"Utilizamos los paquetes leaflet y leaflet.extras (asegurarse de cargar los dos). Por defecto utiliza el mapa OSM (OpenStreetMap), que incluye informaci\u00f3n de carreteras, etc. Para visualizaci\u00f3n es mejor usar un mapa m\u00e1s limpio, como CartoDB. Para incluir nuestros datos en el mapa usar addMarkers, addCircles \u00f3 addHeatmap. leaflet va a buscar la informaci\u00f3n de latitud y longitud en columnas que se llamen latitude y longitude ; si las nuestras tienen un nombre diferente, pasarlas como argumentos con la virgulilla (~). m <- df3 %>% leaflet () %>% addProviderTiles ( \"CartoDB\" ) %>% # Add default OpenStreetMap map tiles addHeatmap ( lng =~ `Longitud (WGS84)` , lat =~ Latitud , radius = 10 ) # addCircles(lng=~`Longitud (WGS84)`, lat=~Latitud, popup=\"Gasolinera\") m # Mostrar el mapa Para m\u00e1s opciones de visualizaci\u00f3n, https://www.htmlwidgets.org .","title":"Mapas"},{"location":"Estad%C3%ADstica/R/#uso-de-memoria","text":"Podemos ver cu\u00e1nta memoria ocupa un objeto de R con object.size(objeto) . Para ver cu\u00e1nto ocupan los datos descargados de Internet con httr , consultar la propiedad content_length en su header. Los resultados vienen en bytes.","title":"Uso de memoria"},{"location":"Estad%C3%ADstica/R/#manipulacion-de-strings","text":"Utilizamos el paquete stringr , incluido en el core tidyverse. La funci\u00f3n str_extract permite buscar texto con expresiones regulares: shopping_list <- c ( \"apples x4\" , \"bag of flour\" , \"bag of sugar\" , \"milk x2\" ) str_extract ( shopping_list , \"\\\\d\" ) Si hay varios substrings que cumplan la expresi\u00f3n regular, s\u00f3lo nos devolver\u00e1 el primero. Para obtenerlos todos, usar str_extract_all .","title":"Manipulaci\u00f3n de strings"}]}